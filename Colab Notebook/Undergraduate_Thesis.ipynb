{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAlrFza8yiui"
      },
      "source": [
        "#Start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrc1lh6mx1S9",
        "outputId": "a21067c5-8606-4c50-b2d6-383987c85baa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzBbQQeHx_gM",
        "outputId": "d22d5dd4-7968-4004-eb69-43dc51e6749b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.4.2)\n",
            "Collecting gradio\n",
            "  Downloading gradio-4.44.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0 (from gradio)\n",
            "  Downloading fastapi-0.115.0-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.3.0 (from gradio)\n",
            "  Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx>=0.24.1 (from gradio)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.24.7)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.5)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.4)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.10-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.6.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.30.6-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (2024.6.1)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.3.0->gradio)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Collecting starlette<0.39.0,>=0.37.2 (from fastapi<1.0->gradio)\n",
            "  Downloading starlette-0.38.6-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-4.44.0-py3-none-any.whl (18.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.0-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.10-py3-none-any.whl (22 kB)\n",
            "Downloading ruff-0.6.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m104.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading uvicorn-0.30.6-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.38.6-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, websockets, tomlkit, semantic-version, ruff, python-multipart, orjson, h11, ffmpy, aiofiles, uvicorn, starlette, httpcore, httpx, fastapi, gradio-client, gradio\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.0 ffmpy-0.4.0 gradio-4.44.0 gradio-client-1.3.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 orjson-3.10.7 pydub-0.25.1 python-multipart-0.0.10 ruff-0.6.7 semantic-version-2.10.0 starlette-0.38.6 tomlkit-0.12.0 uvicorn-0.30.6 websockets-12.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim\n",
        "!pip install transformers\n",
        "!pip install joblib\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sY2csYQl2AhY",
        "outputId": "38302c36-1deb-4be9-a5a7-f31caa8de8c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ekphrasis\n",
            "  Downloading ekphrasis-0.5.4-py3-none-any.whl.metadata (610 bytes)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (2.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (4.66.5)\n",
            "Collecting colorama (from ekphrasis)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting ujson (from ekphrasis)\n",
            "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (3.7.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (3.8.1)\n",
            "Collecting ftfy (from ekphrasis)\n",
            "  Downloading ftfy-6.2.3-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (1.26.4)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->ekphrasis) (0.2.13)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (2.8.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->ekphrasis) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->ekphrasis) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->ekphrasis) (2024.9.11)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->ekphrasis) (1.16.0)\n",
            "Downloading ekphrasis-0.5.4-py3-none-any.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.8/83.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading ftfy-6.2.3-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.0/43.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ujson, ftfy, colorama, ekphrasis\n",
            "Successfully installed colorama-0.4.6 ekphrasis-0.5.4 ftfy-6.2.3 ujson-5.10.0\n",
            "Collecting stanza\n",
            "  Downloading stanza-1.9.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting emoji (from stanza)\n",
            "  Downloading emoji-2.13.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stanza) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (3.20.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from stanza) (2.32.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from stanza) (3.3)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (2.4.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stanza) (4.66.5)\n",
            "Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from stanza) (2.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (1.13.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (2024.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.0->stanza) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n",
            "Downloading stanza-1.9.2-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading emoji-2.13.0-py3-none-any.whl (553 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m553.2/553.2 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji, stanza\n",
            "Successfully installed emoji-2.13.0 stanza-1.9.2\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n",
            "Collecting validator_collection\n",
            "  Downloading validator_collection-1.5.0-py2.py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from validator_collection) (4.23.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->validator_collection) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->validator_collection) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->validator_collection) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->validator_collection) (0.20.0)\n",
            "Downloading validator_collection-1.5.0-py2.py3-none-any.whl (36 kB)\n",
            "Installing collected packages: validator_collection\n",
            "Successfully installed validator_collection-1.5.0\n",
            "Collecting emot\n",
            "  Downloading emot-3.1-py3-none-any.whl.metadata (396 bytes)\n",
            "Downloading emot-3.1-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emot\n",
            "Successfully installed emot-3.1\n",
            "Collecting urlexpander\n",
            "  Downloading urlexpander-0.0.37.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tldextract (from urlexpander)\n",
            "  Downloading tldextract-5.1.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from urlexpander) (2.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from urlexpander) (1.26.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from urlexpander) (4.66.5)\n",
            "Collecting unshortenit (from urlexpander)\n",
            "  Downloading unshortenit-0.4.0.tar.gz (8.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->urlexpander) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->urlexpander) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->urlexpander) (2024.1)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from tldextract->urlexpander) (3.10)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from tldextract->urlexpander) (2.32.3)\n",
            "Collecting requests-file>=1.4 (from tldextract->urlexpander)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract->urlexpander) (3.16.1)\n",
            "Requirement already satisfied: click>=6.7 in /usr/local/lib/python3.10/dist-packages (from unshortenit->urlexpander) (8.1.7)\n",
            "Requirement already satisfied: lxml>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from unshortenit->urlexpander) (4.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->urlexpander) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->urlexpander) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->urlexpander) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->urlexpander) (2024.8.30)\n",
            "Downloading tldextract-5.1.2-py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.6/97.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Building wheels for collected packages: urlexpander, unshortenit\n",
            "  Building wheel for urlexpander (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for urlexpander: filename=urlexpander-0.0.37-py3-none-any.whl size=11097 sha256=f7f0867981f8a2b9c0825880ea25a77a8ab03e697b051537fb9e47abf6344c05\n",
            "  Stored in directory: /root/.cache/pip/wheels/e3/93/7f/397f3bdc773a926ef1b70ea1150f6ffc211f0c1c9c5d3af2f1\n",
            "  Building wheel for unshortenit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unshortenit: filename=unshortenit-0.4.0-py3-none-any.whl size=12394 sha256=b5959bbd9f84db9a08001f07f8bad199e5be51bbe6fc9d4a312f5198b0bc74e3\n",
            "  Stored in directory: /root/.cache/pip/wheels/09/11/2d/9e95c7bca95d2d2c4b296c2c0bf46bc3de0eb65469c3eb0cf2\n",
            "Successfully built urlexpander unshortenit\n",
            "Installing collected packages: unshortenit, requests-file, tldextract, urlexpander\n",
            "Successfully installed requests-file-2.1.0 tldextract-5.1.2 unshortenit-0.4.0 urlexpander-0.0.37\n",
            "Collecting krovetzstemmer\n",
            "  Downloading KrovetzStemmer-0.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.9/112.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: krovetzstemmer\n",
            "  Building wheel for krovetzstemmer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for krovetzstemmer: filename=KrovetzStemmer-0.8-cp310-cp310-linux_x86_64.whl size=373249 sha256=5625f5310ce2c892e1865988d9aa12cdd46738ee8039642c97cafac6e0a8cd44\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/82/b0/1677fc180ec3a9eaa9902ea38650bca7e0211d052a3bd8ce39\n",
            "Successfully built krovetzstemmer\n",
            "Installing collected packages: krovetzstemmer\n",
            "Successfully installed krovetzstemmer-0.8\n",
            "Collecting spellchecker\n",
            "  Downloading spellchecker-0.4.tar.gz (3.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spellchecker) (71.0.4)\n",
            "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/inexactsearch/\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting inexactsearch (from spellchecker)\n",
            "  Downloading inexactsearch-1.0.2.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting soundex>=1.0 (from inexactsearch->spellchecker)\n",
            "  Downloading soundex-1.1.3.tar.gz (9.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting silpa_common>=0.3 (from inexactsearch->spellchecker)\n",
            "  Downloading silpa_common-0.3.tar.gz (9.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: spellchecker, inexactsearch, silpa_common, soundex\n",
            "  Building wheel for spellchecker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spellchecker: filename=spellchecker-0.4-py3-none-any.whl size=3966499 sha256=fc70e540cbdb2c911476c62bd66b10a54e97021e6e83cf2a5eb75abc1d2f8104\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/90/c3/eac248d8755b2a7343487a2087b4b29ad98f388c3c8c69c286\n",
            "  Building wheel for inexactsearch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for inexactsearch: filename=inexactsearch-1.0.2-py3-none-any.whl size=7120 sha256=6bfcbc1a64f03ca481705ba5c3f2c9a9d3dac76df81ee9d10d53bf677b97f3d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/19/2c/5e9f447f2533d457a1167c3e553f235e232b8a639e3f5fafab\n",
            "  Building wheel for silpa_common (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for silpa_common: filename=silpa_common-0.3-py3-none-any.whl size=8469 sha256=7753e75d409fe94a2d43d435ac426c7791fa340d3af202641faa3dc9c582f08c\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/72/43/0c779b79d708c78240beb3b0bb8f5ff3c2ab81c4e5271ea1aa\n",
            "  Building wheel for soundex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for soundex: filename=soundex-1.1.3-py3-none-any.whl size=8875 sha256=d1fed7137da700552a71a68fa430cced4bed35b4146c12ea3314f8fd0883be89\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c7/c0/99e0278924f5664ab201bee9eee6e7a856caabf95a6fe008c5\n",
            "Successfully built spellchecker inexactsearch silpa_common soundex\n",
            "Installing collected packages: silpa_common, soundex, inexactsearch, spellchecker\n",
            "Successfully installed inexactsearch-1.0.2 silpa_common-0.3 soundex-1.1.3 spellchecker-0.4\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (71.0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.26.4)\n",
            "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp310-cp310-linux_x86_64.whl size=4296182 sha256=5e2c91d4049d6ca088c6c02c2aa18db6f1a33e080699cbf151b0596fe99708a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/a2/00/81db54d3e6a8199b829d58e02cec2ddb20ce3e59fad8d3c92a\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.3 pybind11-2.13.6\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.8\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install ekphrasis\n",
        "!pip install stanza\n",
        "!pip install contractions\n",
        "!pip install validator_collection\n",
        "!pip install emot\n",
        "!pip install urlexpander\n",
        "!pip install krovetzstemmer\n",
        "!pip install spellchecker\n",
        "!pip install fasttext\n",
        "!pip install nltk\n",
        "!pip install unidecode\n",
        "!pip install gensim\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XHZfRZ12EVc",
        "outputId": "03a32ee1-e333-45d4-ddcf-3b797bee6fa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_rus.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/maxent_treebank_pos_tagger_tab.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets_json.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('all')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMTIC5GokjRz"
      },
      "source": [
        "# Bert+Fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 507,
          "referenced_widgets": [
            "6c95488e5d08488db44f3ced1514a0f6",
            "1c5d7e3bff2f4b8d906858492a10568f",
            "80594434db724cf691cde24cb424ba70",
            "562bb5fed45942c3998146c28f0ebec7",
            "b698f79d9a6a48d5ab6c73a564b04bf2",
            "999f85da94d248018c655b518f78e3bb",
            "8af9878d5fa141c3b19cb19e13508fe2",
            "1d33620fb1e840db8be0eb642e2dcf29",
            "23189e118de4495eb98035b14496d92f",
            "f2e38f8002494959baa138e8bd6daf4d",
            "27bb3370327b41a796941ae385955a3d",
            "b3067447ea2b400ca2afe284ebbcbee1",
            "9a7fb4d27878421c970e11d8e7b1333a",
            "944fd0cf6dc64769b5443a9c1c6b7af1",
            "4dc716daf9bc4c98be01cfcf35b4067d",
            "c10ce5367cc747bdabae7c46941d421a",
            "5dc51983f8104ab4b903ddb9902f6015",
            "f2075f9507c3432e8f430256a857739b",
            "6acb83090aca40e1aa45f53c05003818",
            "407093cf3ac14872849c8fae711ee35e",
            "9035289b8988466ea6311011d73ed585",
            "749c7d7eb9344323921bae86ab05268f",
            "211b04c9c15446bbbb88ff9e4c9208b3",
            "0d712dbc37f14c3ea21923cece4c91d2",
            "fa65980d6d234f79a98a950e1a4b8b41",
            "9a038f2aa1e9441da197886d019a49c6",
            "469c05b9d53a4441a0a872ac5ea927fe",
            "473f38a269834c369f33af407f9d73d3",
            "8b97e6b8a432480598b4d3e32e2e7a19",
            "87c52b363d234080bee5e8267b4836e7",
            "928906f15d8c4245b5438b9b302774f4",
            "d12d4f1fabf4457f97f13fc37de3a7b2",
            "033b523b938d4720be2173a8107d7acf",
            "278da640f40947b992f4fadf35de8b98",
            "60ca1128414c468aaca980aa762ef753",
            "f116126af6ad4c239278e304a78c5425",
            "6594e621fd304deaad83c0ca536b12fe",
            "3563b45366174366a3b482ea6e797e0c",
            "9f97281a11fe47a0a92ba705bb2c4e9a",
            "c9c28c06babd40838aff9a21b8673944",
            "eb55153ab1db454fb9c2444e8e9888b2",
            "0d0c19deb2284e78bd69528b4df03005",
            "1c245728fb3641e3bc7d72608deba63c",
            "dfdf8c79bf6b4f40b57a85fd631455bd",
            "af1b06e2f91e4020ab6a38536a62fb4e",
            "e85c7798ee1b42f9984cddeddf0a2c9c",
            "50680eceddb2412580f1b5466a0dba9f",
            "905563cf8f7a4398aad87d80dafd8dff",
            "39d3fc4d5ab34fa7893a4c4d12ec77c3",
            "ffd3286c8c474acfac736ecc91c17c5d",
            "8e03812b15bd4f539f0a566d8fbd0b07",
            "6f5641ec9e584d9daf20ba5a8bdc372e",
            "0d2584d0fab84abf8800b28505349684",
            "7613983e6f43489d80e95fa193828be0",
            "81829d9430084411b12c6c7ab4c41844"
          ]
        },
        "id": "Xc9LwHsOkoAA",
        "outputId": "dcd352db-8068-4f6e-d06c-ab3d464ce7be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word statistics files not found!\n",
            "Downloading... done!\n",
            "Unpacking... done!\n",
            "Reading twitter - 1grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/twitter/counts_1grams.txt\n",
            "Reading twitter - 2grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/twitter/counts_2grams.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
            "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c95488e5d08488db44f3ced1514a0f6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b3067447ea2b400ca2afe284ebbcbee1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "211b04c9c15446bbbb88ff9e4c9208b3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "278da640f40947b992f4fadf35de8b98",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af1b06e2f91e4020ab6a38536a62fb4e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "[==================================================] 100.0% 958.5/958.4MB downloaded\n"
          ]
        }
      ],
      "source": [
        "# Import Section\n",
        "import csv\n",
        "import sys\n",
        "import joblib\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics import *\n",
        "import gensim.downloader as api\n",
        "import gradio as gr\n",
        "\n",
        "# Import preprocessing module\n",
        "sys.path.append(\"/content/drive/MyDrive/Colab Notebooks/Project2022\")\n",
        "import PreprocessingModule as cpm\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-large-uncased')\n",
        "\n",
        "# Set device to GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Move BERT model to GPU\n",
        "bert_model = bert_model.to(device)\n",
        "\n",
        "# Load pre-trained FastText word embeddings\n",
        "fasttext_model = api.load(\"fasttext-wiki-news-subwords-300\")\n",
        "\n",
        "def fasttext_embeddings(texts):\n",
        "    embeddings_list = []\n",
        "    for text in texts:\n",
        "        text=cpm.preProcessingModule(text)\n",
        "        #text=text.lower()\n",
        "        words = text.split()\n",
        "        # Compute the average FastText word embedding for the words in the text\n",
        "        embeddings = [fasttext_model[word] for word in words if word in fasttext_model]\n",
        "        if embeddings:\n",
        "            embeddings = np.mean(embeddings, axis=0)\n",
        "        else:\n",
        "            # Handle missing words by using zero vectors\n",
        "            embeddings = np.zeros(fasttext_model.vector_size)\n",
        "        embeddings_list.append(embeddings)\n",
        "    return torch.tensor(embeddings_list)\n",
        "\n",
        "def bert_embeddings(texts, batch_size=16):\n",
        "    # Tokenize text\n",
        "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
        "\n",
        "    # Move inputs to device (GPU or CPU)\n",
        "    for key in inputs:\n",
        "        inputs[key] = inputs[key].to(device)\n",
        "\n",
        "    # Create DataLoader\n",
        "    data_loader = torch.utils.data.DataLoader(list(range(len(texts))), batch_size=batch_size)\n",
        "\n",
        "    pooled_embeddings_list = []\n",
        "\n",
        "    # Iterate over the data loader\n",
        "    with torch.no_grad():\n",
        "        for batch_indices in data_loader:\n",
        "\n",
        "            batch_inputs = {key: inputs[key][batch_indices] for key in inputs}\n",
        "            # Pass the tokenized input through BERT model\n",
        "            outputs = bert_model(**batch_inputs)\n",
        "            # Obtain embeddings from BERT model\n",
        "            embeddings = outputs.last_hidden_state\n",
        "            # Perform mean pooling\n",
        "            pooled_embeddings = torch.mean(embeddings, dim=1)\n",
        "\n",
        "            pooled_embeddings_list.append(pooled_embeddings.cpu())\n",
        "\n",
        "    # Concatenate pooled embeddings from all batches\n",
        "    pooled_embeddings = torch.cat(pooled_embeddings_list, dim=0)\n",
        "\n",
        "    return pooled_embeddings\n",
        "\n",
        "def main():\n",
        "    tweets = []\n",
        "    label = []\n",
        "    label1= []\n",
        "    label2= []\n",
        "\n",
        "    with open('/content/drive/MyDrive/Colab Notebooks/Thesis/train_en.txt', 'r') as f:\n",
        "        next(f) # skip headings\n",
        "        reader = csv.reader(f, dialect=\"excel-tab\")\n",
        "        for line in reader:\n",
        "            preProcessedTweetText = line[1]\n",
        "            tweets.append(preProcessedTweetText)\n",
        "            if line[2] == \"1\":\n",
        "                label.append(\"hateful\")\n",
        "            else:\n",
        "                label.append(\"notHateful\")\n",
        "\n",
        "            if(line[3]==\"1\"):\n",
        "\n",
        "              label1.append(\"individual\")\n",
        "            else:\n",
        "              label1.append(\"generic\")\n",
        "\n",
        "            if(line[4]==\"1\"):\n",
        "\n",
        "              label2.append(\"aggressive\")\n",
        "            else:\n",
        "              label2.append(\"notAggressive\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    f.close()\n",
        "\n",
        "    X_train = tweets\n",
        "    Y_train = np.array(label)\n",
        "    Y_train1 = np.array(label1)\n",
        "    Y_train2 = np.array(label2)\n",
        "\n",
        "    # Get BERT embeddings for training data\n",
        "    X_train_bert = bert_embeddings(X_train)\n",
        "\n",
        "    ## Get FastText embeddings for training data\n",
        "    X_train_fasttext = fasttext_embeddings(X_train)\n",
        "\n",
        "    ## Combine FastText and BERT embeddings\n",
        "    X_train_combined = torch.cat((X_train_bert, X_train_fasttext), dim=1)\n",
        "\n",
        "    classifier = Pipeline([\n",
        "        ('clf', MLPClassifier(hidden_layer_sizes=(10,), max_iter=32, alpha=1e-1, activation='relu',\n",
        "                              solver='adam', beta_1=0.44, beta_2=0.999,\n",
        "                              random_state=0, batch_size= 128 , epsilon=1e-5))\n",
        "    ])\n",
        "\n",
        "    classifier1 = Pipeline([\n",
        "        ('clf', MLPClassifier(hidden_layer_sizes=(10,), max_iter=32, alpha=1e-1, activation='relu',\n",
        "                              solver='adam', #beta_1=0.44, beta_2=0.999,\n",
        "                              random_state=0, batch_size= 128 , epsilon=1e-5))\n",
        "    ])\n",
        "\n",
        "    classifier2 = Pipeline([\n",
        "        ('clf', MLPClassifier(hidden_layer_sizes=(10,), max_iter=32, alpha=1e-1, activation='relu',\n",
        "                              solver='adam', #beta_1=0.44, beta_2=0.999,\n",
        "                              random_state=0, batch_size= 128 , epsilon=1e-5))\n",
        "    ])\n",
        "\n",
        "    ## Train Classifier\n",
        "    classifier.fit(X_train_combined, Y_train)\n",
        "\n",
        "    classifier1.fit(X_train_combined, Y_train1)\n",
        "\n",
        "    classifier2.fit(X_train_combined, Y_train2)\n",
        "\n",
        "    #model_filename = 'trained_classifier_model.pkl'\n",
        "    #joblib.dump(classifier, model_filename)\n",
        "\n",
        "\n",
        "\n",
        "    ## Test Set Prediction Module\n",
        "    testTweets = []\n",
        "    testLabelGold = []\n",
        "    testLabelGold1= []\n",
        "    testLabelGold2= []\n",
        "\n",
        "    with open('/content/drive/MyDrive/Colab Notebooks/Thesis/test.txt', 'r') as f:\n",
        "        next(f)\n",
        "        reader = csv.reader(f, dialect=\"excel-tab\")\n",
        "        for line in reader:\n",
        "            preProcessedTweetText = line[1]\n",
        "            testTweets.append(preProcessedTweetText)\n",
        "            if line[2] == \"1\":\n",
        "                testLabelGold.append(\"hateful\")\n",
        "            else:\n",
        "                testLabelGold.append(\"notHateful\")\n",
        "\n",
        "            if(line[3] == \"1\"):\n",
        "                testLabelGold1.append(\"individual\")\n",
        "            else:\n",
        "                testLabelGold1.append(\"generic\")\n",
        "\n",
        "            if(line[4] == \"1\"):\n",
        "                testLabelGold2.append(\"aggressive\")\n",
        "            else:\n",
        "                testLabelGold2.append(\"notAggressive\")\n",
        "\n",
        "\n",
        "    f.close()\n",
        "\n",
        "    print(\"Complete test data reading .....\")\n",
        "    X_test = testTweets\n",
        "\n",
        "    # Get BERT embeddings for test data\n",
        "    X_test_bert = bert_embeddings(X_test)\n",
        "\n",
        "    ## Get FastText embeddings for test data\n",
        "    X_test_fasttext = fasttext_embeddings(X_test)\n",
        "\n",
        "    ## Combine FastText and BERT embeddings\n",
        "    X_test_combined = np.concatenate((X_test_bert.cpu().numpy(), X_test_fasttext), axis=1)\n",
        "\n",
        "    # Test data prediction\n",
        "    testLabelPredicted = classifier.predict(X_test_combined)\n",
        "\n",
        "    testLabelPredicted1 = classifier1.predict(X_test_combined)\n",
        "\n",
        "    testLabelPredicted2 = classifier2.predict(X_test_combined)\n",
        "\n",
        "    # Evaluation\n",
        "    results = confusion_matrix(testLabelGold, testLabelPredicted)\n",
        "\n",
        "    results1 = confusion_matrix(testLabelGold1, testLabelPredicted1)\n",
        "\n",
        "    results2 = confusion_matrix(testLabelGold2, testLabelPredicted2)\n",
        "\n",
        "    # with open('/content/drive/MyDrive/Colab Notebooks/Thesis/test.txt', 'r') as f1:\n",
        "    #   with open('/content/drive/MyDrive/Colab Notebooks/Thesis/test1.txt', 'w') as file1:\n",
        "    #     next(f1)\n",
        "    #     file1.write(\"id\"+\"\\t\"+\"text\"+\"\\t\"+\"HS\"+\"\\t\"+\"TR\"+\"\\t\"+\"AG\"+\"\\n\")\n",
        "    #     reader=csv.reader(f1, dialect=\"excel-tab\")\n",
        "    #     cnt=0\n",
        "    #     for line in reader:\n",
        "    #       if(testLabelPredicted[cnt]=='hateful'):\n",
        "    #         k=1\n",
        "    #       else:\n",
        "    #         k=0\n",
        "    #       file1.write(line[0]+\"\\t\"+line[1]+\"\\t\"+str(k)+\"\\t\"+line[3]+\"\\t\"+line[4]+\"\\n\")\n",
        "    #       cnt=cnt+1\n",
        "    #     print(cnt)\n",
        "    # f1.close()\n",
        "    # file1.close()\n",
        "\n",
        "\n",
        "    print('Confusion Matrix of HS:')\n",
        "    print(results)\n",
        "\n",
        "    print('Recall Score:', recall_score(testLabelGold, testLabelPredicted, average=\"macro\"))\n",
        "    print('Precision Score:', precision_score(testLabelGold, testLabelPredicted, average=\"macro\"))\n",
        "    print('F1 Score:', f1_score(testLabelGold, testLabelPredicted, average=\"macro\"))\n",
        "    print('Accuracy:', accuracy_score(testLabelGold, testLabelPredicted))\n",
        "\n",
        "\n",
        "    print('Confusion Matrix of TR:')\n",
        "    print(results1)\n",
        "\n",
        "    print('Recall Score:', recall_score(testLabelGold1, testLabelPredicted1, average=\"macro\"))\n",
        "    print('Precision Score:', precision_score(testLabelGold1, testLabelPredicted1, average=\"macro\"))\n",
        "    print('F1 Score:', f1_score(testLabelGold1, testLabelPredicted1, average=\"macro\"))\n",
        "    print('Accuracy:', accuracy_score(testLabelGold1, testLabelPredicted1))\n",
        "\n",
        "\n",
        "    print('Confusion Matrix of AG:')\n",
        "    print(results2)\n",
        "\n",
        "    print('Recall Score:', recall_score(testLabelGold2, testLabelPredicted2, average=\"macro\"))\n",
        "    print('Precision Score:', precision_score(testLabelGold2, testLabelPredicted2, average=\"macro\"))\n",
        "    print('F1 Score:', f1_score(testLabelGold2, testLabelPredicted2, average=\"macro\"))\n",
        "    print('Accuracy:', accuracy_score(testLabelGold2, testLabelPredicted2))\n",
        "\n",
        "\n",
        "    def classify_tweet(tweet):\n",
        "        tweet_bert = bert_embeddings([tweet])\n",
        "        tweet_fasttext = fasttext_embeddings([tweet])\n",
        "        tweet_combined = torch.cat((tweet_bert, tweet_fasttext), dim=1)\n",
        "\n",
        "        label_predicted = classifier.predict(tweet_combined)[0]\n",
        "\n",
        "        if label_predicted == 'hateful':\n",
        "            tweet_label = 'hateful'\n",
        "            label1_predicted = classifier1.predict(tweet_combined)[0]\n",
        "            label2_predicted = classifier2.predict(tweet_combined)[0]\n",
        "\n",
        "            if label1_predicted == 'individual':\n",
        "                tweet_label1 = 'individual'\n",
        "            else:\n",
        "                tweet_label1 = 'generic'\n",
        "\n",
        "            if label2_predicted == 'aggressive':\n",
        "                tweet_label2 = 'aggressive'\n",
        "            else:\n",
        "                tweet_label2 = 'notAggressive'\n",
        "\n",
        "            return f\"Predicted Label1: Hateful\\n,Predicted Label2: {tweet_label1}\\n,Predicted Label3: {tweet_label2}\\n\"\n",
        "\n",
        "        else:\n",
        "            return \"Predicted Label: Not Hateful\"\n",
        "\n",
        "    # Define the Gradio Interface\n",
        "    interface = gr.Interface(\n",
        "        fn=classify_tweet,\n",
        "        inputs=gr.inputs.Textbox(lines=2, placeholder=\"Enter tweet text...\"),\n",
        "        outputs=gr.outputs.Textbox(),\n",
        "        title=\"Hate Speech Detection in Twitter Focusing on Aggressiveness and Target Classification\",\n",
        "        description=\"Enter a tweet and get predictions\"\n",
        "    )\n",
        "\n",
        "\n",
        "    interface.launch(share=True)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6pzCkCnz-40"
      },
      "source": [
        "# Thesis-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qouD9ljCko4E"
      },
      "outputs": [],
      "source": [
        "# Import Section\n",
        "import csv\n",
        "import sys\n",
        "import numpy as np\n",
        "import torch\n",
        "import joblib\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics import *\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Import feature extraction module\n",
        "sys.path.append(\"/content/drive/MyDrive/Colab Notebooks/Project2022\")\n",
        "import PreprocessingModule as cpm\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-large-uncased')\n",
        "\n",
        "# Set device to GPU if available, else use CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Move BERT model to GPU\n",
        "bert_model = bert_model.to(device)\n",
        "\n",
        "# Load pre-trained FastText word embeddings\n",
        "fasttext_model = api.load(\"fasttext-wiki-news-subwords-300\")\n",
        "\n",
        "def get_fasttext_embeddings(texts):\n",
        "    embeddings_list = []\n",
        "    for text in texts:\n",
        "        # Tokenize the text into words\n",
        "        #text=text.lower()\n",
        "        text=cpm.preProcessingModule(text)\n",
        "        words = text.split()\n",
        "        # Compute the average FastText word embedding for the words in the text\n",
        "        embeddings = [fasttext_model[word] for word in words if word in fasttext_model]\n",
        "        if embeddings:\n",
        "            embeddings = np.mean(embeddings, axis=0)\n",
        "        else:\n",
        "            # Handle missing words by using zero vectors\n",
        "            embeddings = np.zeros(fasttext_model.vector_size)\n",
        "        embeddings_list.append(embeddings)\n",
        "    return torch.tensor(embeddings_list)\n",
        "\n",
        "def get_bert_embeddings(texts, batch_size=16):\n",
        "    # Tokenize text\n",
        "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
        "\n",
        "    # Move inputs to device (GPU or CPU)\n",
        "    for key in inputs:\n",
        "        inputs[key] = inputs[key].to(device)\n",
        "\n",
        "    # Create DataLoader with the specified batch size\n",
        "    data_loader = torch.utils.data.DataLoader(list(range(len(texts))), batch_size=batch_size)\n",
        "\n",
        "    # List to store pooled embeddings\n",
        "    pooled_embeddings_list = []\n",
        "\n",
        "    # Iterate over the data loader\n",
        "    with torch.no_grad():\n",
        "        for batch_indices in data_loader:\n",
        "            # Get the batched input tensors using indices\n",
        "            batch_inputs = {key: inputs[key][batch_indices] for key in inputs}\n",
        "            # Pass the tokenized input through BERT model\n",
        "            outputs = bert_model(**batch_inputs)\n",
        "            # Obtain embeddings from BERT model\n",
        "            embeddings = outputs.last_hidden_state\n",
        "            # Perform pooling (e.g., mean pooling) to obtain fixed-size representation\n",
        "            pooled_embeddings = torch.mean(embeddings, dim=1)  # You can use other pooling strategies\n",
        "            # Move the pooled embeddings to the CPU and append to the list\n",
        "            pooled_embeddings_list.append(pooled_embeddings.cpu())\n",
        "\n",
        "    # Concatenate pooled embeddings from all batches to obtain the final embeddings\n",
        "    pooled_embeddings = torch.cat(pooled_embeddings_list, dim=0)\n",
        "\n",
        "    return pooled_embeddings\n",
        "\n",
        "def main():\n",
        "    tweets = []\n",
        "    label = []\n",
        "\n",
        "    with open('/content/drive/MyDrive/Colab Notebooks/Thesis/train_en.txt', 'r') as f:\n",
        "        next(f) # skip headings\n",
        "        reader = csv.reader(f, dialect=\"excel-tab\")\n",
        "        for line in reader:\n",
        "            preProcessedTweetText = line[1]\n",
        "            #if (line[2]==\"1\"):\n",
        "            tweets.append(preProcessedTweetText)\n",
        "            if(line[4]==\"1\"):\n",
        "              #cnt=cnt+1\n",
        "              label.append(\"aggressive\")\n",
        "            else:\n",
        "              label.append(\"notAggressive\")\n",
        "\n",
        "    f.close()\n",
        "\n",
        "    X_train = tweets\n",
        "    Y_train = np.array(label)\n",
        "\n",
        "    # Get BERT embeddings for training data\n",
        "    X_train_bert = get_bert_embeddings(X_train)\n",
        "\n",
        "    ## Get FastText embeddings for training data\n",
        "    X_train_fasttext = get_fasttext_embeddings(X_train)\n",
        "\n",
        "    ## Combine FastText and BERT embeddings\n",
        "    X_train_combined = torch.cat((X_train_bert, X_train_fasttext), dim=1)\n",
        "\n",
        "    classifier = Pipeline([\n",
        "        ('clf', MLPClassifier(hidden_layer_sizes=(10,), max_iter=32, alpha=1e-1, activation='relu',\n",
        "                              solver='adam', #early_stopping=True,\n",
        "                              random_state=0, batch_size= 128, epsilon=1e-5))\n",
        "    ])\n",
        "\n",
        "    ## Train Classifier\n",
        "    classifier.fit(X_train_combined, Y_train)\n",
        "\n",
        "    ## Test Set Prediction Module\n",
        "    testTweets = []\n",
        "    testLabelGold = []\n",
        "\n",
        "    with open('/content/drive/MyDrive/Colab Notebooks/Thesis/test2.txt', 'r') as f:\n",
        "        next(f)\n",
        "        reader = csv.reader(f, dialect=\"excel-tab\")\n",
        "        for line in reader:\n",
        "            preProcessedTweetText = line[1]\n",
        "            testTweets.append(preProcessedTweetText)\n",
        "            if (line[2]==\"1\" and line[4] == \"1\"):\n",
        "                testLabelGold.append(\"aggressive\")\n",
        "            else:\n",
        "                testLabelGold.append(\"notAggressive\")\n",
        "    f.close()\n",
        "\n",
        "    print(\"Complete test data reading .....\")\n",
        "    X_test = testTweets\n",
        "\n",
        "    # Get BERT embeddings for test data\n",
        "    X_test_bert = get_bert_embeddings(X_test)\n",
        "\n",
        "    ## Get FastText embeddings for test data\n",
        "    X_test_fasttext = get_fasttext_embeddings(X_test)\n",
        "\n",
        "    ## Combine FastText and BERT embeddings\n",
        "    X_test_combined = np.concatenate((X_test_bert.cpu().numpy(), X_test_fasttext), axis=1)\n",
        "\n",
        "    # Test data prediction\n",
        "    testLabelPredicted = classifier.predict(X_test_combined)\n",
        "\n",
        "    # Evaluation\n",
        "    results = confusion_matrix(testLabelGold, testLabelPredicted)\n",
        "\n",
        "    with open('/content/drive/MyDrive/Colab Notebooks/Thesis/test2.txt', 'r') as f1:\n",
        "      with open('/content/drive/MyDrive/Colab Notebooks/Thesis/test3.txt', 'w') as file1:\n",
        "        next(f1)\n",
        "        file1.write(\"id\"+\"\\t\"+\"HS\"+\"\\t\"+\"TR\"+\"\\t\"+\"AG\"+\"\\n\")\n",
        "        reader=csv.reader(f1, dialect=\"excel-tab\")\n",
        "        cnt=0\n",
        "        for line in reader:\n",
        "          if(testLabelPredicted[cnt]=='aggressive'):\n",
        "            k=1\n",
        "          else:\n",
        "            k=0\n",
        "          file1.write(line[0]+\"\\t\"+line[2]+\"\\t\"+line[3]+\"\\t\"+str(k)+\"\\n\")\n",
        "          cnt=cnt+1\n",
        "        print(cnt)\n",
        "    f1.close()\n",
        "    file1.close()\n",
        "\n",
        "    print('Confusion Matrix:')\n",
        "    print(results)\n",
        "\n",
        "    print('Recall Score:', recall_score(testLabelGold, testLabelPredicted, average=\"macro\"))\n",
        "    print('Precision Score:', precision_score(testLabelGold, testLabelPredicted, average=\"macro\"))\n",
        "    print('F1 Score:', f1_score(testLabelGold, testLabelPredicted, average=\"macro\"))\n",
        "    print('Accuracy:', accuracy_score(testLabelGold, testLabelPredicted))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xuhfnVE3qaK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Step 1: Load and preprocess the data\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Thesis/PTrain.txt', delimiter='\\t')\n",
        "dev_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Thesis/PDev.txt', delimiter='\\t')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Thesis/PTest.txt', delimiter='\\t')\n",
        "\n",
        "train_tweets = train_data['text'].values\n",
        "train_labels = train_data['HS'].values\n",
        "dev_tweets = dev_data['text'].values\n",
        "dev_labels = dev_data['HS'].values\n",
        "test_tweets = test_data['text'].values\n",
        "test_labels = test_data['HS'].values\n",
        "\n",
        "# Step 2: Fine-tune the transformer model (BERT)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "\n",
        "train_encodings = tokenizer(list(train_tweets), padding=True, truncation=True, return_tensors='tf')\n",
        "dev_encodings = tokenizer(list(dev_tweets), padding=True, truncation=True, return_tensors='tf')\n",
        "test_encodings = tokenizer(list(test_tweets), padding=True, truncation=True, return_tensors='tf')\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "# Perform fine-tuning with validation on development data\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    train_labels\n",
        ")).shuffle(len(train_data)).batch(16)\n",
        "\n",
        "dev_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(dev_encodings),\n",
        "    dev_labels\n",
        ")).batch(16)\n",
        "\n",
        "model.fit(train_dataset, epochs=5, validation_data=dev_dataset)\n",
        "\n",
        "# Step 3: Build and train a deep learning classifier\n",
        "train_logits = model.predict(dict(train_encodings))['logits']\n",
        "dev_logits = model.predict(dict(dev_encodings))['logits']\n",
        "\n",
        "classifier = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(train_logits.shape[1],)),\n",
        "    Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "classifier.fit(train_logits, train_labels, epochs=5, batch_size=32)\n",
        "\n",
        "# Step 4: Evaluate the model\n",
        "test_logits = model.predict(dict(test_encodings))['logits']\n",
        "predictions = classifier.predict(test_logits)\n",
        "binary_predictions = np.round(predictions).astype(int)\n",
        "\n",
        "macro_f1 = f1_score(test_labels, binary_predictions, average='macro')\n",
        "accuracy = accuracy_score(test_labels, binary_predictions)\n",
        "precision = precision_score(test_labels, binary_predictions, average='macro')\n",
        "recall = recall_score(test_labels, binary_predictions, average='macro')\n",
        "\n",
        "print(\"Macro F1:\", macro_f1)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ke67XcuykodU"
      },
      "outputs": [],
      "source": [
        "# Import Section\n",
        "import csv\n",
        "import sys\n",
        "import numpy as np\n",
        "import joblib\n",
        "import torch\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics import *\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Import feature extraction module\n",
        "sys.path.append(\"/content/drive/MyDrive/Colab Notebooks/Project2022\")\n",
        "import PreprocessingModule as cpm\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-large-uncased')\n",
        "\n",
        "# Set device to GPU if available, else use CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Move BERT model to GPU\n",
        "bert_model = bert_model.to(device)\n",
        "\n",
        "# Load pre-trained FastText word embeddings\n",
        "fasttext_model = api.load(\"fasttext-wiki-news-subwords-300\")\n",
        "\n",
        "def get_fasttext_embeddings(texts):\n",
        "    embeddings_list = []\n",
        "    for text in texts:\n",
        "        # Tokenize the text into words\n",
        "        text=cpm.preProcessingModule(text)\n",
        "        #text=text.lower()\n",
        "        words = text.split()\n",
        "        # Compute the average FastText word embedding for the words in the text\n",
        "        embeddings = [fasttext_model[word] for word in words if word in fasttext_model]\n",
        "        if embeddings:\n",
        "            embeddings = np.mean(embeddings, axis=0)\n",
        "        else:\n",
        "            # Handle missing words by using zero vectors\n",
        "            embeddings = np.zeros(fasttext_model.vector_size)\n",
        "        embeddings_list.append(embeddings)\n",
        "    return torch.tensor(embeddings_list)\n",
        "\n",
        "def get_bert_embeddings(texts, batch_size=16):\n",
        "    # Tokenize text\n",
        "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
        "\n",
        "    # Move inputs to device (GPU or CPU)\n",
        "    for key in inputs:\n",
        "        inputs[key] = inputs[key].to(device)\n",
        "\n",
        "    # Create DataLoader with the specified batch size\n",
        "    data_loader = torch.utils.data.DataLoader(list(range(len(texts))), batch_size=batch_size)\n",
        "\n",
        "    # List to store pooled embeddings\n",
        "    pooled_embeddings_list = []\n",
        "\n",
        "    # Iterate over the data loader\n",
        "    with torch.no_grad():\n",
        "        for batch_indices in data_loader:\n",
        "            # Get the batched input tensors using indices\n",
        "            batch_inputs = {key: inputs[key][batch_indices] for key in inputs}\n",
        "            # Pass the tokenized input through BERT model\n",
        "            outputs = bert_model(**batch_inputs)\n",
        "            # Obtain embeddings from BERT model\n",
        "            embeddings = outputs.last_hidden_state\n",
        "            # Perform pooling (e.g., mean pooling) to obtain fixed-size representation\n",
        "            pooled_embeddings = torch.mean(embeddings, dim=1)  # You can use other pooling strategies\n",
        "            # Move the pooled embeddings to the CPU and append to the list\n",
        "            pooled_embeddings_list.append(pooled_embeddings.cpu())\n",
        "\n",
        "    # Concatenate pooled embeddings from all batches to obtain the final embeddings\n",
        "    pooled_embeddings = torch.cat(pooled_embeddings_list, dim=0)\n",
        "\n",
        "    return pooled_embeddings\n",
        "\n",
        "def main():\n",
        "    tweets = []\n",
        "    label = []\n",
        "\n",
        "    with open('/content/drive/MyDrive/Colab Notebooks/Thesis/train_en.txt', 'r') as f:\n",
        "        next(f) # skip headings\n",
        "        reader = csv.reader(f, dialect=\"excel-tab\")\n",
        "        for line in reader:\n",
        "            preProcessedTweetText = line[1]\n",
        "            #if (line[2]==\"1\"):\n",
        "            tweets.append(preProcessedTweetText)\n",
        "            if(line[3]==\"1\"):\n",
        "              #cnt=cnt+1\n",
        "              label.append(\"individual\")\n",
        "            else:\n",
        "              label.append(\"generic\")\n",
        "\n",
        "    f.close()\n",
        "\n",
        "    X_train = tweets\n",
        "    Y_train = np.array(label)\n",
        "\n",
        "    # Get BERT embeddings for training data\n",
        "    X_train_bert = get_bert_embeddings(X_train)\n",
        "\n",
        "    ## Get FastText embeddings for training data\n",
        "    X_train_fasttext = get_fasttext_embeddings(X_train)\n",
        "\n",
        "    ## Combine FastText and BERT embeddings\n",
        "    X_train_combined = torch.cat((X_train_bert, X_train_fasttext), dim=1)\n",
        "\n",
        "    classifier = Pipeline([\n",
        "        ('clf', MLPClassifier(hidden_layer_sizes=(10,), max_iter=32, alpha=1e-1, activation='relu',\n",
        "                              solver='adam',\n",
        "                              random_state=0, batch_size= 128, epsilon=1e-5))\n",
        "    ])\n",
        "\n",
        "    ## Train Classifier\n",
        "    classifier.fit(X_train_combined, Y_train)\n",
        "\n",
        "    model_filename = 'trained_classifier_model1.pkl'\n",
        "    joblib.dump(classifier, model_filename)\n",
        "\n",
        "    print('Classifier saved as', model_filename)\n",
        "\n",
        "    ## Test Set Prediction Module\n",
        "    testTweets = []\n",
        "    testLabelGold = []\n",
        "\n",
        "    with open('/content/drive/MyDrive/Colab Notebooks/Thesis/test1.txt', 'r') as f:\n",
        "        next(f)\n",
        "        reader = csv.reader(f, dialect=\"excel-tab\")\n",
        "        for line in reader:\n",
        "            preProcessedTweetText = line[1]\n",
        "            testTweets.append(preProcessedTweetText)\n",
        "            if(line[3] == \"1\"):\n",
        "                testLabelGold.append(\"individual\")\n",
        "            else:\n",
        "                testLabelGold.append(\"generic\")\n",
        "    f.close()\n",
        "\n",
        "    print(\"Complete test data reading .....\")\n",
        "    X_test = testTweets\n",
        "\n",
        "    # Get BERT embeddings for test data\n",
        "    X_test_bert = get_bert_embeddings(X_test)\n",
        "\n",
        "    ## Get FastText embeddings for test data\n",
        "    X_test_fasttext = get_fasttext_embeddings(X_test)\n",
        "\n",
        "    ## Combine FastText and BERT embeddings\n",
        "    X_test_combined = np.concatenate((X_test_bert.cpu().numpy(), X_test_fasttext), axis=1)\n",
        "\n",
        "    # Test data prediction\n",
        "    testLabelPredicted = classifier.predict(X_test_combined)\n",
        "\n",
        "    # Evaluation\n",
        "    results = confusion_matrix(testLabelGold, testLabelPredicted)\n",
        "\n",
        "    with open('/content/drive/MyDrive/Colab Notebooks/Thesis/test1.txt', 'r') as f1:\n",
        "      with open('/content/drive/MyDrive/Colab Notebooks/Thesis/test2.txt', 'w') as file1:\n",
        "        next(f1)\n",
        "        file1.write(\"id\"+\"\\t\"+\"text\"+\"\\t\"+\"HS\"+\"\\t\"+\"TR\"+\"\\t\"+\"AG\"+\"\\n\")\n",
        "        reader=csv.reader(f1, dialect=\"excel-tab\")\n",
        "        cnt=0\n",
        "        for line in reader:\n",
        "          if(testLabelPredicted[cnt]=='individual'):\n",
        "            k=1\n",
        "          else:\n",
        "            k=0\n",
        "          file1.write(line[0]+\"\\t\"+line[1]+\"\\t\"+line[2]+\"\\t\"+str(k)+\"\\t\"+line[4]+\"\\n\")\n",
        "          cnt=cnt+1\n",
        "        print(cnt)\n",
        "    f1.close()\n",
        "    file1.close()\n",
        "\n",
        "    print('Confusion Matrix:')\n",
        "    print(results)\n",
        "\n",
        "    print('Recall Score:', recall_score(testLabelGold, testLabelPredicted, average=\"macro\"))\n",
        "    print('Precision Score:', precision_score(testLabelGold, testLabelPredicted, average=\"macro\"))\n",
        "    print('F1 Score:', f1_score(testLabelGold, testLabelPredicted, average=\"macro\"))\n",
        "    print('Accuracy:', accuracy_score(testLabelGold, testLabelPredicted))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqRMvujWlBnZ"
      },
      "source": [
        "# MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtGmMqlHlEux"
      },
      "outputs": [],
      "source": [
        "# Import Section\n",
        "import csv\n",
        "import codecs\n",
        "import sys\n",
        "import io\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import imp\n",
        "import importlib\n",
        "import sklearn\n",
        "\n",
        "# Import preprocessing module\n",
        "#import CustomizedPreprocessingModule as cpm\n",
        "\n",
        "# Import feature union module for pipeline design\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "# For Classifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Python script for confusion matrix creation.\n",
        "from sklearn.metrics import *\n",
        "\n",
        "\n",
        "def main():\n",
        "  tweets = []\n",
        "  label = []\n",
        "  #cpm.lexical_dictionary()\n",
        "  csv.field_size_limit(500 * 1024 * 1024)\n",
        "  with open('/content/drive/MyDrive/Colab Notebooks/Thesis/train_en.txt', 'r') as f:\n",
        "  #with open('/content/drive/My Drive/Colab Notebooks/SampleProject/SemEval2018-IronyDetection.txt', 'r') as f:\n",
        "      next(f) # skip headings\n",
        "      reader=csv.reader(f, dialect=\"excel-tab\")\n",
        "      for line in reader:\n",
        "          #print(line[2])\n",
        "          #preProcessedTweetText= cpm.preProcessingModule(line[1])\n",
        "          preProcessedTweetText = line[1]\n",
        "          #print(preProcessedTweetText)\n",
        "          tweets.append(preProcessedTweetText)\n",
        "          if(line[2]==\"1\"):\n",
        "            label.append(\"hateful\")\n",
        "          else:\n",
        "            label.append(\"notHateful\")\n",
        "      print(label)\n",
        "  f.close()\n",
        "\n",
        "\n",
        "\n",
        "  X_train = np.array(tweets)\n",
        "  Y_train = np.array(label)\n",
        "\n",
        "  classifier = Pipeline([\n",
        "    ('features', FeatureUnion([\n",
        "          ('text', Pipeline([\n",
        "        ('tfidf_vectorizer', TfidfVectorizer(ngram_range=(1,1),binary=True))\n",
        "                  ])),\n",
        "\n",
        "\n",
        "     ])),\n",
        "\n",
        "     ('clf', MLPClassifier())])\n",
        "\n",
        "\n",
        "     #('clf', LinearSVC(kernel = 'rbf'))])\n",
        "\n",
        "\n",
        "  ## Train Classifier\n",
        "  classifier.fit(X_train, Y_train)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  ## Test Set Prediction Module\n",
        "  testTweets = []\n",
        "  testLabelGold = []\n",
        "  csv.field_size_limit(500 * 1024 * 1024)\n",
        "  with open('/content/drive/MyDrive/Colab Notebooks/Thesis/test.txt', 'r') as f:\n",
        "  #with open('/content/drive/My Drive/Colab Notebooks/SampleProject/SemEval2018-IronyDetection_Gold_Test.txt', 'r') as f:\n",
        "      next(f) # skip headings\n",
        "      reader=csv.reader(f, dialect=\"excel-tab\")\n",
        "      for line in reader:\n",
        "          #print(line[2])\n",
        "          preProcessedTweetText=line[1]\n",
        "          #preProcessedTweetText = line[1]\n",
        "          #print(preProcessedTweetText)\n",
        "          testTweets.append(preProcessedTweetText)\n",
        "          #print(preProcessedTweetText)\n",
        "          if(line[2]==\"1\"):\n",
        "            testLabelGold.append(\"hateful\")\n",
        "          else:\n",
        "            testLabelGold.append(\"notHateful\")\n",
        "  f.close()\n",
        "\n",
        "  print(\"Complete test data reading .....\")\n",
        "  X_test = np.array(testTweets)\n",
        "\n",
        "  # Test data prediction\n",
        "  testLabelPredicted = classifier.predict(X_test)\n",
        "  print(testLabelPredicted)\n",
        "  #print(testLabelPredicted)\n",
        "\n",
        "  results = confusion_matrix(testLabelGold, testLabelPredicted)\n",
        "\n",
        "  print ('Confusion Matrix :')\n",
        "  print (results)\n",
        "\n",
        "  print ('Recall Score :',recall_score(testLabelGold, testLabelPredicted,average=\"macro\"))\n",
        "  print ('Precision Score :',precision_score(testLabelGold, testLabelPredicted,average=\"macro\"))\n",
        "  print ('F1 Score :',f1_score(testLabelGold, testLabelPredicted,average=\"macro\"))\n",
        "  print ('Accuracy :',accuracy_score(testLabelGold, testLabelPredicted))\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvbQANnelLY-"
      },
      "outputs": [],
      "source": [
        "# Import Section\n",
        "import csv\n",
        "import codecs\n",
        "import sys\n",
        "import io\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import imp\n",
        "import importlib\n",
        "import sklearn\n",
        "\n",
        "# Import preprocessing module\n",
        "#import CustomizedPreprocessingModule as cpm\n",
        "\n",
        "\n",
        "# Import feature union module for pipeline design\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "# For Classifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Python script for confusion matrix creation.\n",
        "from sklearn.metrics import *\n",
        "\n",
        "\n",
        "def main():\n",
        "  tweets = []\n",
        "  label = []\n",
        "  cnt=0\n",
        "  csv.field_size_limit(500 * 1024 * 1024)\n",
        "  with open('/content/drive/MyDrive/Colab Notebooks/Thesis/train_en.txt', 'r') as f:\n",
        "  #with open('/content/drive/My Drive/Colab Notebooks/SampleProject/SemEval2018-IronyDetection.txt', 'r') as f:\n",
        "      next(f) # skip headings\n",
        "      reader=csv.reader(f, dialect=\"excel-tab\")\n",
        "      for line in reader:\n",
        "          #print(line[3])\n",
        "          #preProcessedTweetText= cpm.preProcessingModule(line[1])\n",
        "          preProcessedTweetText = line[1]\n",
        "          #print(preProcessedTweetText)\n",
        "          #if (line[2]==\"1\"):\n",
        "          tweets.append(preProcessedTweetText)\n",
        "          if(line[3]==\"1\"):\n",
        "            #cnt=cnt+1\n",
        "            label.append(\"individual\")\n",
        "          elif(line[3]==\"0\"):\n",
        "            label.append(\"generic\")\n",
        "      print(len(tweets))\n",
        "  f.close()\n",
        "\n",
        "\n",
        "\n",
        "  X_train = np.array(tweets)\n",
        "  Y_train = np.array(label)\n",
        "\n",
        "  classifier = Pipeline([\n",
        "    ('features', FeatureUnion([\n",
        "          ('text', Pipeline([\n",
        "        ('tfidf_vectorizer', TfidfVectorizer(ngram_range=(1, 1),binary=True))\n",
        "                  ])),\n",
        "\n",
        "\n",
        "     ])),\n",
        "    ('clf', MLPClassifier())])\n",
        "\n",
        "\n",
        "  ## Train Classifier\n",
        "  classifier.fit(X_train, Y_train)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  ## Test Set Prediction Module\n",
        "  testTweets = []\n",
        "  testLabelGold = []\n",
        "  csv.field_size_limit(500 * 1024 * 1024)\n",
        "  with open('/content/drive/MyDrive/Colab Notebooks/Thesis/test.txt', 'r') as f:\n",
        "  #with open('/content/drive/My Drive/Colab Notebooks/SampleProject/SemEval2018-IronyDetection_Gold_Test.txt', 'r') as f:\n",
        "      next(f) # skip headings\n",
        "      reader=csv.reader(f, dialect=\"excel-tab\")\n",
        "      for line in reader:\n",
        "          #print(line[2])\n",
        "          #preProcessedTweetText= cpm.preProcessingModule(line[1])\n",
        "          preProcessedTweetText = line[1]\n",
        "          #print(preProcessedTweetText)\n",
        "          #if(line[2]==\"1\"):\n",
        "          testTweets.append(preProcessedTweetText)\n",
        "          #print(preProcessedTweetText)\n",
        "          if(line[3]==\"1\"):\n",
        "            testLabelGold.append(\"individual\")\n",
        "          else:\n",
        "            testLabelGold.append(\"generic\")\n",
        "  f.close()\n",
        "\n",
        "  print(\"Complete test data reading .....\")\n",
        "  X_test = np.array(testTweets)\n",
        "\n",
        "  # Test data prediction\n",
        "  testLabelPredicted = classifier.predict(X_test)\n",
        "  print(testLabelPredicted)\n",
        "  #print(testLabelPredicted)\n",
        "\n",
        "  #Evaluation\n",
        "  results = confusion_matrix(testLabelGold, testLabelPredicted)\n",
        "\n",
        "  print ('Confusion Matrix :')\n",
        "  print (results)\n",
        "\n",
        "  print ('Recall Score :',recall_score(testLabelGold, testLabelPredicted,average=\"macro\"))\n",
        "  print ('Precision Score :',precision_score(testLabelGold, testLabelPredicted,average=\"macro\"))\n",
        "  print ('F1 Score :',f1_score(testLabelGold, testLabelPredicted,average=\"macro\"))\n",
        "  print ('Accuracy :',accuracy_score(testLabelGold, testLabelPredicted))\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbjek9iklNzR"
      },
      "outputs": [],
      "source": [
        "# Import Section\n",
        "import csv\n",
        "import codecs\n",
        "import sys\n",
        "import io\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import imp\n",
        "import importlib\n",
        "import sklearn\n",
        "\n",
        "# Import preprocessing module\n",
        "#import CustomizedPreprocessingModule as cpm\n",
        "\n",
        "# Import feature union module for pipeline design\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "# For Classifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Python script for confusion matrix creation.\n",
        "from sklearn.metrics import *\n",
        "\n",
        "\n",
        "def main():\n",
        "  tweets = []\n",
        "  label = []\n",
        "  csv.field_size_limit(500 * 1024 * 1024)\n",
        "  with open('/content/drive/MyDrive/Colab Notebooks/Thesis/train_en.txt', 'r') as f:\n",
        "  #with open('/content/drive/My Drive/Colab Notebooks/SampleProject/SemEval2018-IronyDetection.txt', 'r') as f:\n",
        "      next(f) # skip headings\n",
        "      reader=csv.reader(f, dialect=\"excel-tab\")\n",
        "      for line in reader:\n",
        "          #print(line[2])\n",
        "          #preProcessedTweetText= cpm.preProcessingModule(line[1])\n",
        "          preProcessedTweetText = line[1]\n",
        "          #print(preProcessedTweetText)\n",
        "          #if(line[2]==\"1\"):\n",
        "          tweets.append(preProcessedTweetText)\n",
        "          if(line[4]==\"1\"):\n",
        "            label.append(\"aggressive\")\n",
        "          elif(line[4]==\"0\"):\n",
        "            label.append(\"notaggressive\")\n",
        "      #print(label)\n",
        "  f.close()\n",
        "\n",
        "\n",
        "\n",
        "  X_train = np.array(tweets)\n",
        "  Y_train = np.array(label)\n",
        "\n",
        "  classifier = Pipeline([\n",
        "    ('features', FeatureUnion([\n",
        "          ('text', Pipeline([\n",
        "        ('tfidf_vectorizer', TfidfVectorizer())\n",
        "                  ])),\n",
        "\n",
        "     ])),\n",
        "    ('clf', MLPClassifier())])\n",
        "\n",
        "\n",
        "  ## Train Classifier\n",
        "  classifier.fit(X_train, Y_train)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  ## Test Set Prediction Module\n",
        "  testTweets = []\n",
        "  testLabelGold = []\n",
        "  csv.field_size_limit(500 * 1024 * 1024)\n",
        "  with open('/content/drive/MyDrive/Colab Notebooks/Thesis/test.txt', 'r') as f:\n",
        "  #with open('/content/drive/My Drive/Colab Notebooks/SampleProject/SemEval2018-IronyDetection_Gold_Test.txt', 'r') as f:\n",
        "      next(f) # skip headings\n",
        "      reader=csv.reader(f, dialect=\"excel-tab\")\n",
        "      for line in reader:\n",
        "          #print(line[2])\n",
        "          #preProcessedTweetText= cpm.preProcessingModule(line[1])\n",
        "          preProcessedTweetText = line[1]\n",
        "          #print(preProcessedTweetText)\n",
        "          #if(line[2]==\"1\"):\n",
        "          testTweets.append(preProcessedTweetText)\n",
        "          #print(preProcessedTweetText)\n",
        "          if(line[4]==\"1\"):\n",
        "            testLabelGold.append(\"aggressive\")\n",
        "          else:\n",
        "            testLabelGold.append(\"notaggressive\")\n",
        "  f.close()\n",
        "\n",
        "  print(\"Complete test data reading .....\")\n",
        "  X_test = np.array(testTweets)\n",
        "\n",
        "  # Test data prediction\n",
        "  testLabelPredicted = classifier.predict(X_test)\n",
        "  print(testLabelPredicted)\n",
        "  #print(testLabelPredicted)\n",
        "\n",
        "  #Evaluation\n",
        "  results = confusion_matrix(testLabelGold, testLabelPredicted)\n",
        "\n",
        "  print ('Confusion Matrix :')\n",
        "  print (results)\n",
        "\n",
        "  print ('Recall Score :',recall_score(testLabelGold, testLabelPredicted,average=\"macro\"))\n",
        "  print ('Precision Score :',precision_score(testLabelGold, testLabelPredicted,average=\"macro\"))\n",
        "  print ('F1 Score :',f1_score(testLabelGold, testLabelPredicted, average=\"macro\"))\n",
        "  print ('Accuracy :',accuracy_score(testLabelGold, testLabelPredicted))\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6Wlt1hzuGjo"
      },
      "source": [
        "# Develop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BhYuprNul3w"
      },
      "outputs": [],
      "source": [
        "# Import Section\n",
        "import csv\n",
        "import sys\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics import *\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Import feature extraction module\n",
        "sys.path.append(\"/content/drive/MyDrive/Colab Notebooks/Project2022\")\n",
        "import PreprocessingModule as cpm\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-large-uncased')\n",
        "\n",
        "# Set device to GPU if available, else use CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Move BERT model to GPU\n",
        "bert_model = bert_model.to(device)\n",
        "\n",
        "# Load pre-trained FastText word embeddings\n",
        "fasttext_model = api.load(\"fasttext-wiki-news-subwords-300\")\n",
        "\n",
        "def get_fasttext_embeddings(texts):\n",
        "    embeddings_list = []\n",
        "    for text in texts:\n",
        "        # Tokenize the text into words\n",
        "        #text=text.lower()\n",
        "        text=cpm.preProcessingModule(text)\n",
        "        words = text.split()\n",
        "        # Compute the average FastText word embedding for the words in the text\n",
        "        embeddings = [fasttext_model[word] for word in words if word in fasttext_model]\n",
        "        if embeddings:\n",
        "            embeddings = np.mean(embeddings, axis=0)\n",
        "        else:\n",
        "            # Handle missing words by using zero vectors\n",
        "            embeddings = np.zeros(fasttext_model.vector_size)\n",
        "        embeddings_list.append(embeddings)\n",
        "    return torch.tensor(embeddings_list)\n",
        "\n",
        "def get_bert_embeddings(texts, batch_size=16):\n",
        "    # Tokenize text\n",
        "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
        "\n",
        "    # Move inputs to device (GPU or CPU)\n",
        "    for key in inputs:\n",
        "        inputs[key] = inputs[key].to(device)\n",
        "\n",
        "    # Create DataLoader with the specified batch size\n",
        "    data_loader = torch.utils.data.DataLoader(list(range(len(texts))), batch_size=batch_size)\n",
        "\n",
        "    # List to store pooled embeddings\n",
        "    pooled_embeddings_list = []\n",
        "\n",
        "    # Iterate over the data loader\n",
        "    with torch.no_grad():\n",
        "        for batch_indices in data_loader:\n",
        "            # Get the batched input tensors using indices\n",
        "            batch_inputs = {key: inputs[key][batch_indices] for key in inputs}\n",
        "            # Pass the tokenized input through BERT model\n",
        "            outputs = bert_model(**batch_inputs)\n",
        "            # Obtain embeddings from BERT model\n",
        "            embeddings = outputs.last_hidden_state\n",
        "            # Perform pooling (e.g., mean pooling) to obtain fixed-size representation\n",
        "            pooled_embeddings = torch.mean(embeddings, dim=1)  # You can use other pooling strategies\n",
        "            # Move the pooled embeddings to the CPU and append to the list\n",
        "            pooled_embeddings_list.append(pooled_embeddings.cpu())\n",
        "\n",
        "    # Concatenate pooled embeddings from all batches to obtain the final embeddings\n",
        "    pooled_embeddings = torch.cat(pooled_embeddings_list, dim=0)\n",
        "\n",
        "    return pooled_embeddings\n",
        "\n",
        "def main():\n",
        "    tweets = []\n",
        "    label = []\n",
        "\n",
        "    with open('/content/drive/MyDrive/Colab Notebooks/Thesis/train_en.txt', 'r') as f:\n",
        "        next(f) # skip headings\n",
        "        reader = csv.reader(f, dialect=\"excel-tab\")\n",
        "        for line in reader:\n",
        "            preProcessedTweetText = line[1]\n",
        "            tweets.append(preProcessedTweetText)\n",
        "            if line[2] == \"1\":\n",
        "                label.append(\"hateful\")\n",
        "            else:\n",
        "                label.append(\"notHateful\")\n",
        "\n",
        "    f.close()\n",
        "\n",
        "    X_train = tweets\n",
        "    Y_train = np.array(label)\n",
        "\n",
        "    # Get BERT embeddings for training data\n",
        "    X_train_bert = get_bert_embeddings(X_train)\n",
        "\n",
        "    ## Get FastText embeddings for training data\n",
        "    X_train_fasttext = get_fasttext_embeddings(X_train)\n",
        "\n",
        "    ## Combine FastText and BERT embeddings\n",
        "    X_train_combined = torch.cat((X_train_bert, X_train_fasttext), dim=1)\n",
        "\n",
        "    classifier = Pipeline([\n",
        "        ('clf', MLPClassifier(hidden_layer_sizes=(10,), max_iter=32, alpha=1e-1, activation='relu',\n",
        "                              solver='adam', beta_1=0.44, beta_2=0.999,\n",
        "                              random_state=0, batch_size= 128, epsilon=1e-5))\n",
        "    ])\n",
        "\n",
        "    ## Train Classifier\n",
        "    classifier.fit(X_train_combined, Y_train)\n",
        "\n",
        "    ## Test Set Prediction Module\n",
        "    testTweets = []\n",
        "    testLabelGold = []\n",
        "\n",
        "    with open('/content/drive/MyDrive/Colab Notebooks/Thesis/dev_en.txt', 'r') as f:\n",
        "        next(f)\n",
        "        reader = csv.reader(f, dialect=\"excel-tab\")\n",
        "        for line in reader:\n",
        "            preProcessedTweetText = line[1]\n",
        "            testTweets.append(preProcessedTweetText)\n",
        "            if line[2] == \"1\":\n",
        "                testLabelGold.append(\"hateful\")\n",
        "            else:\n",
        "                testLabelGold.append(\"notHateful\")\n",
        "    f.close()\n",
        "\n",
        "    print(\"Complete test data reading .....\")\n",
        "    X_test = testTweets\n",
        "\n",
        "    # Get BERT embeddings for test data\n",
        "    X_test_bert = get_bert_embeddings(X_test)\n",
        "\n",
        "    ## Get FastText embeddings for test data\n",
        "    X_test_fasttext = get_fasttext_embeddings(X_test)\n",
        "\n",
        "    ## Combine FastText and BERT embeddings\n",
        "    X_test_combined = np.concatenate((X_test_bert.cpu().numpy(), X_test_fasttext), axis=1)\n",
        "\n",
        "    # Test data prediction\n",
        "    testLabelPredicted = classifier.predict(X_test_combined)\n",
        "\n",
        "    # Evaluation\n",
        "    results = confusion_matrix(testLabelGold, testLabelPredicted)\n",
        "\n",
        "    print('Confusion Matrix:')\n",
        "    print(results)\n",
        "\n",
        "    print('Recall Score:', recall_score(testLabelGold, testLabelPredicted, average=\"macro\"))\n",
        "    print('Precision Score:', precision_score(testLabelGold, testLabelPredicted, average=\"macro\"))\n",
        "    print('F1 Score:', f1_score(testLabelGold, testLabelPredicted, average=\"macro\"))\n",
        "    print('Accuracy:', accuracy_score(testLabelGold, testLabelPredicted))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJIX7c-zuUsa"
      },
      "outputs": [],
      "source": [
        "# Import Section\n",
        "import csv\n",
        "import sys\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics import *\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Import feature extraction module\n",
        "sys.path.append(\"/content/drive/MyDrive/Colab Notebooks/Project2022\")\n",
        "import PreprocessingModule as cpm\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-large-uncased')\n",
        "\n",
        "# Set device to GPU if available, else use CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Move BERT model to GPU\n",
        "bert_model = bert_model.to(device)\n",
        "\n",
        "# Load pre-trained FastText word embeddings\n",
        "fasttext_model = api.load(\"fasttext-wiki-news-subwords-300\")\n",
        "\n",
        "def get_fasttext_embeddings(texts):\n",
        "    embeddings_list = []\n",
        "    for text in texts:\n",
        "        # Tokenize the text into words\n",
        "        #text=text.lower()\n",
        "        text=cpm.preProcessingModule(text)\n",
        "        words = text.split()\n",
        "        # Compute the average FastText word embedding for the words in the text\n",
        "        embeddings = [fasttext_model[word] for word in words if word in fasttext_model]\n",
        "        if embeddings:\n",
        "            embeddings = np.mean(embeddings, axis=0)\n",
        "        else:\n",
        "            # Handle missing words by using zero vectors\n",
        "            embeddings = np.zeros(fasttext_model.vector_size)\n",
        "        embeddings_list.append(embeddings)\n",
        "    return torch.tensor(embeddings_list)\n",
        "\n",
        "def get_bert_embeddings(texts, batch_size=16):\n",
        "    # Tokenize text\n",
        "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
        "\n",
        "    # Move inputs to device (GPU or CPU)\n",
        "    for key in inputs:\n",
        "        inputs[key] = inputs[key].to(device)\n",
        "\n",
        "    # Create DataLoader with the specified batch size\n",
        "    data_loader = torch.utils.data.DataLoader(list(range(len(texts))), batch_size=batch_size)\n",
        "\n",
        "    # List to store pooled embeddings\n",
        "    pooled_embeddings_list = []\n",
        "\n",
        "    # Iterate over the data loader\n",
        "    with torch.no_grad():\n",
        "        for batch_indices in data_loader:\n",
        "            # Get the batched input tensors using indices\n",
        "            batch_inputs = {key: inputs[key][batch_indices] for key in inputs}\n",
        "            # Pass the tokenized input through BERT model\n",
        "            outputs = bert_model(**batch_inputs)\n",
        "            # Obtain embeddings from BERT model\n",
        "            embeddings = outputs.last_hidden_state\n",
        "            # Perform pooling (e.g., mean pooling) to obtain fixed-size representation\n",
        "            pooled_embeddings = torch.mean(embeddings, dim=1)  # You can use other pooling strategies\n",
        "            # Move the pooled embeddings to the CPU and append to the list\n",
        "            pooled_embeddings_list.append(pooled_embeddings.cpu())\n",
        "\n",
        "    # Concatenate pooled embeddings from all batches to obtain the final embeddings\n",
        "    pooled_embeddings = torch.cat(pooled_embeddings_list, dim=0)\n",
        "\n",
        "    return pooled_embeddings\n",
        "\n",
        "def main():\n",
        "    tweets = []\n",
        "    label = []\n",
        "\n",
        "    with open('/content/drive/MyDrive/Colab Notebooks/Thesis/train_en.txt', 'r') as f:\n",
        "        next(f) # skip headings\n",
        "        reader = csv.reader(f, dialect=\"excel-tab\")\n",
        "        for line in reader:\n",
        "            preProcessedTweetText = line[1]\n",
        "            #if (line[2]==\"1\"):\n",
        "            tweets.append(preProcessedTweetText)\n",
        "            if(line[3]==\"1\"):\n",
        "              #cnt=cnt+1\n",
        "              label.append(\"individual\")\n",
        "            elif(line[3]==\"0\"):\n",
        "              label.append(\"generic\")\n",
        "\n",
        "    f.close()\n",
        "\n",
        "    X_train = tweets\n",
        "    Y_train = np.array(label)\n",
        "\n",
        "    # Get BERT embeddings for training data\n",
        "    X_train_bert = get_bert_embeddings(X_train)\n",
        "\n",
        "    ## Get FastText embeddings for training data\n",
        "    X_train_fasttext = get_fasttext_embeddings(X_train)\n",
        "\n",
        "    ## Combine FastText and BERT embeddings\n",
        "    X_train_combined = torch.cat((X_train_bert, X_train_fasttext), dim=1)\n",
        "\n",
        "    classifier = Pipeline([\n",
        "        ('clf', MLPClassifier(hidden_layer_sizes=(10,), max_iter=32, alpha=1e-1, activation='relu',\n",
        "                              solver='adam',\n",
        "                              random_state=0, batch_size= 128, epsilon=1e-5))\n",
        "    ])\n",
        "\n",
        "    ## Train Classifier\n",
        "    classifier.fit(X_train_combined, Y_train)\n",
        "\n",
        "    ## Test Set Prediction Module\n",
        "    testTweets = []\n",
        "    testLabelGold = []\n",
        "\n",
        "    with open('/content/drive/MyDrive/Colab Notebooks/Thesis/dev_en.txt', 'r') as f:\n",
        "        next(f)\n",
        "        reader = csv.reader(f, dialect=\"excel-tab\")\n",
        "        for line in reader:\n",
        "            preProcessedTweetText = line[1]\n",
        "            testTweets.append(preProcessedTweetText)\n",
        "            if line[3] == \"1\":\n",
        "                testLabelGold.append(\"individual\")\n",
        "            else:\n",
        "                testLabelGold.append(\"generic\")\n",
        "    f.close()\n",
        "\n",
        "    print(\"Complete test data reading .....\")\n",
        "    X_test = testTweets\n",
        "\n",
        "    # Get BERT embeddings for test data\n",
        "    X_test_bert = get_bert_embeddings(X_test)\n",
        "\n",
        "    ## Get FastText embeddings for test data\n",
        "    X_test_fasttext = get_fasttext_embeddings(X_test)\n",
        "\n",
        "    ## Combine FastText and BERT embeddings\n",
        "    X_test_combined = np.concatenate((X_test_bert.cpu().numpy(), X_test_fasttext), axis=1)\n",
        "\n",
        "    # Test data prediction\n",
        "    testLabelPredicted = classifier.predict(X_test_combined)\n",
        "\n",
        "    # Evaluation\n",
        "    results = confusion_matrix(testLabelGold, testLabelPredicted)\n",
        "\n",
        "    print('Confusion Matrix:')\n",
        "    print(results)\n",
        "\n",
        "    print('Recall Score:', recall_score(testLabelGold, testLabelPredicted, average=\"macro\"))\n",
        "    print('Precision Score:', precision_score(testLabelGold, testLabelPredicted, average=\"macro\"))\n",
        "    print('F1 Score:', f1_score(testLabelGold, testLabelPredicted, average=\"macro\"))\n",
        "    print('Accuracy:', accuracy_score(testLabelGold, testLabelPredicted))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arK9eLfvuLsg"
      },
      "outputs": [],
      "source": [
        "# Import Section\n",
        "import csv\n",
        "import sys\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics import *\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Import feature extraction module\n",
        "sys.path.append(\"/content/drive/MyDrive/Colab Notebooks/Project2022\")\n",
        "import PreprocessingModule as cpm\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-large-uncased')\n",
        "\n",
        "# Set device to GPU if available, else use CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Move BERT model to GPU\n",
        "bert_model = bert_model.to(device)\n",
        "\n",
        "# Load pre-trained FastText word embeddings\n",
        "fasttext_model = api.load(\"fasttext-wiki-news-subwords-300\")\n",
        "\n",
        "def get_fasttext_embeddings(texts):\n",
        "    embeddings_list = []\n",
        "    for text in texts:\n",
        "        # Tokenize the text into words\n",
        "        #text=text.lower()\n",
        "        text=cpm.preProcessingModule(text)\n",
        "        words = text.split()\n",
        "        # Compute the average FastText word embedding for the words in the text\n",
        "        embeddings = [fasttext_model[word] for word in words if word in fasttext_model]\n",
        "        if embeddings:\n",
        "            embeddings = np.mean(embeddings, axis=0)\n",
        "        else:\n",
        "            # Handle missing words by using zero vectors\n",
        "            embeddings = np.zeros(fasttext_model.vector_size)\n",
        "        embeddings_list.append(embeddings)\n",
        "    return torch.tensor(embeddings_list)\n",
        "\n",
        "def get_bert_embeddings(texts, batch_size=16):\n",
        "    # Tokenize text\n",
        "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
        "\n",
        "    # Move inputs to device (GPU or CPU)\n",
        "    for key in inputs:\n",
        "        inputs[key] = inputs[key].to(device)\n",
        "\n",
        "    # Create DataLoader with the specified batch size\n",
        "    data_loader = torch.utils.data.DataLoader(list(range(len(texts))), batch_size=batch_size)\n",
        "\n",
        "    # List to store pooled embeddings\n",
        "    pooled_embeddings_list = []\n",
        "\n",
        "    # Iterate over the data loader\n",
        "    with torch.no_grad():\n",
        "        for batch_indices in data_loader:\n",
        "            # Get the batched input tensors using indices\n",
        "            batch_inputs = {key: inputs[key][batch_indices] for key in inputs}\n",
        "            # Pass the tokenized input through BERT model\n",
        "            outputs = bert_model(**batch_inputs)\n",
        "            # Obtain embeddings from BERT model\n",
        "            embeddings = outputs.last_hidden_state\n",
        "            # Perform pooling (e.g., mean pooling) to obtain fixed-size representation\n",
        "            pooled_embeddings = torch.mean(embeddings, dim=1)  # You can use other pooling strategies\n",
        "            # Move the pooled embeddings to the CPU and append to the list\n",
        "            pooled_embeddings_list.append(pooled_embeddings.cpu())\n",
        "\n",
        "    # Concatenate pooled embeddings from all batches to obtain the final embeddings\n",
        "    pooled_embeddings = torch.cat(pooled_embeddings_list, dim=0)\n",
        "\n",
        "    return pooled_embeddings\n",
        "\n",
        "def main():\n",
        "    tweets = []\n",
        "    label = []\n",
        "\n",
        "    with open('/content/drive/MyDrive/Colab Notebooks/Thesis/train_en.txt', 'r') as f:\n",
        "        next(f) # skip headings\n",
        "        reader = csv.reader(f, dialect=\"excel-tab\")\n",
        "        for line in reader:\n",
        "            preProcessedTweetText = line[1]\n",
        "            #if (line[2]==\"1\"):\n",
        "            tweets.append(preProcessedTweetText)\n",
        "            if(line[4]==\"1\"):\n",
        "              #cnt=cnt+1\n",
        "              label.append(\"aggressive\")\n",
        "            else:\n",
        "              label.append(\"notAggressive\")\n",
        "\n",
        "    f.close()\n",
        "\n",
        "    X_train = tweets\n",
        "    Y_train = np.array(label)\n",
        "\n",
        "    # Get BERT embeddings for training data\n",
        "    X_train_bert = get_bert_embeddings(X_train)\n",
        "\n",
        "    ## Get FastText embeddings for training data\n",
        "    X_train_fasttext = get_fasttext_embeddings(X_train)\n",
        "\n",
        "    ## Combine FastText and BERT embeddings\n",
        "    X_train_combined = torch.cat((X_train_bert, X_train_fasttext), dim=1)\n",
        "\n",
        "    classifier = Pipeline([\n",
        "        ('clf', MLPClassifier(hidden_layer_sizes=(10,), max_iter=32, alpha=1e-1, activation='relu',\n",
        "                              solver='adam', #early_stopping=True,\n",
        "                              random_state=0, batch_size= 128, epsilon=1e-5))\n",
        "    ])\n",
        "\n",
        "    ## Train Classifier\n",
        "    classifier.fit(X_train_combined, Y_train)\n",
        "\n",
        "    ## Test Set Prediction Module\n",
        "    testTweets = []\n",
        "    testLabelGold = []\n",
        "\n",
        "    with open('/content/drive/MyDrive/Colab Notebooks/Thesis/dev_en.txt', 'r') as f:\n",
        "        next(f)\n",
        "        reader = csv.reader(f, dialect=\"excel-tab\")\n",
        "        for line in reader:\n",
        "            preProcessedTweetText = line[1]\n",
        "            testTweets.append(preProcessedTweetText)\n",
        "            if line[4] == \"1\":\n",
        "                testLabelGold.append(\"aggressive\")\n",
        "            else:\n",
        "                testLabelGold.append(\"notAggressive\")\n",
        "    f.close()\n",
        "\n",
        "    print(\"Complete test data reading .....\")\n",
        "    X_test = testTweets\n",
        "\n",
        "    # Get BERT embeddings for test data\n",
        "    X_test_bert = get_bert_embeddings(X_test)\n",
        "\n",
        "    ## Get FastText embeddings for test data\n",
        "    X_test_fasttext = get_fasttext_embeddings(X_test)\n",
        "\n",
        "    ## Combine FastText and BERT embeddings\n",
        "    X_test_combined = np.concatenate((X_test_bert.cpu().numpy(), X_test_fasttext), axis=1)\n",
        "\n",
        "    # Test data prediction\n",
        "    testLabelPredicted = classifier.predict(X_test_combined)\n",
        "\n",
        "    # Evaluation\n",
        "    results = confusion_matrix(testLabelGold, testLabelPredicted)\n",
        "\n",
        "    print('Confusion Matrix:')\n",
        "    print(results)\n",
        "\n",
        "    print('Recall Score:', recall_score(testLabelGold, testLabelPredicted, average=\"macro\"))\n",
        "    print('Precision Score:', precision_score(testLabelGold, testLabelPredicted, average=\"macro\"))\n",
        "    print('F1 Score:', f1_score(testLabelGold, testLabelPredicted, average=\"macro\"))\n",
        "    print('Accuracy:', accuracy_score(testLabelGold, testLabelPredicted))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nVwzY10vvkG"
      },
      "source": [
        "# BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRh7x8LUvyq1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b6347f0-c758-486c-da16-5f32dd333cf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Complete test data reading .....\n",
            "Confusion Matrix:\n",
            "[[1075  177]\n",
            " [1119  601]]\n",
            "Recall Score: 0.6040224013671149\n",
            "Precision Score: 0.6312331129769668\n",
            "F1 Score: 0.5525483648671694\n",
            "Accuracy: 0.5639300134589502\n"
          ]
        }
      ],
      "source": [
        "# Import Section\n",
        "import csv\n",
        "import sys\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics import *\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-large-uncased')\n",
        "\n",
        "# Set device to GPU if available, else use CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Move BERT model to GPU\n",
        "bert_model = bert_model.to(device)\n",
        "\n",
        "def get_bert_embeddings(texts, batch_size=8):\n",
        "    # Tokenize text\n",
        "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
        "\n",
        "    # Move inputs to device (GPU or CPU)\n",
        "    for key in inputs:\n",
        "        inputs[key] = inputs[key].to(device)\n",
        "\n",
        "    # Create DataLoader with the specified batch size\n",
        "    data_loader = torch.utils.data.DataLoader(list(range(len(texts))), batch_size=batch_size)\n",
        "\n",
        "    # List to store pooled embeddings\n",
        "    pooled_embeddings_list = []\n",
        "\n",
        "    # Iterate over the data loader\n",
        "    with torch.no_grad():\n",
        "        for batch_indices in data_loader:\n",
        "            # Get the batched input tensors using indices\n",
        "            batch_inputs = {key: inputs[key][batch_indices] for key in inputs}\n",
        "            # Pass the tokenized input through BERT model\n",
        "            outputs = bert_model(**batch_inputs)\n",
        "            # Obtain embeddings from BERT model\n",
        "            embeddings = outputs.last_hidden_state\n",
        "            # Perform pooling (e.g., mean pooling) to obtain fixed-size representation\n",
        "            pooled_embeddings = torch.mean(embeddings, dim=1)  # You can use other pooling strategies\n",
        "            # Move the pooled embeddings to the CPU and append to the list\n",
        "            pooled_embeddings_list.append(pooled_embeddings.cpu())\n",
        "\n",
        "    # Concatenate pooled embeddings from all batches to obtain the final embeddings\n",
        "    pooled_embeddings = torch.cat(pooled_embeddings_list, dim=0)\n",
        "\n",
        "    return pooled_embeddings\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    tweets = []\n",
        "    label = []\n",
        "\n",
        "    with open('/content/drive/MyDrive/Colab Notebooks/Thesis/train_en.txt', 'r') as f:\n",
        "        next(f) # skip headings\n",
        "        reader = csv.reader(f, dialect=\"excel-tab\")\n",
        "        for line in reader:\n",
        "            preProcessedTweetText = line[1]\n",
        "            tweets.append(preProcessedTweetText)\n",
        "            if line[2] == \"1\":\n",
        "                label.append(\"hateful\")\n",
        "            else:\n",
        "                label.append(\"notHateful\")\n",
        "\n",
        "    f.close()\n",
        "\n",
        "    X_train = tweets\n",
        "    Y_train = np.array(label)\n",
        "\n",
        "    # Get BERT embeddings for training data\n",
        "    X_train_bert = get_bert_embeddings(X_train)\n",
        "\n",
        "    classifier = Pipeline([\n",
        "        ('clf', MLPClassifier())\n",
        "    ])\n",
        "\n",
        "\n",
        "    ## Train Classifier\n",
        "    classifier.fit(X_train_bert, Y_train)\n",
        "\n",
        "    ## Test Set Prediction Module\n",
        "    testTweets = []\n",
        "    testLabelGold = []\n",
        "\n",
        "    with open('/content/drive/MyDrive/Colab Notebooks/Thesis/test.txt', 'r') as f:\n",
        "        next(f) # skip headings\n",
        "        reader = csv.reader(f, dialect=\"excel-tab\")\n",
        "        for line in reader:\n",
        "            preProcessedTweetText = line[1]\n",
        "            testTweets.append(preProcessedTweetText)\n",
        "            if line[2] == \"1\":\n",
        "                testLabelGold.append(\"hateful\")\n",
        "            else:\n",
        "                testLabelGold.append(\"notHateful\")\n",
        "    f.close()\n",
        "\n",
        "    print(\"Complete test data reading .....\")\n",
        "    X_test = testTweets\n",
        "\n",
        "    # Get BERT embeddings for test data\n",
        "    X_test_bert = get_bert_embeddings(X_test)\n",
        "\n",
        "    # Test data prediction\n",
        "    testLabelPredicted = classifier.predict(X_test_bert)\n",
        "\n",
        "    # Evaluation\n",
        "    results = confusion_matrix(testLabelGold, testLabelPredicted)\n",
        "\n",
        "    print('Confusion Matrix:')\n",
        "    print(results)\n",
        "\n",
        "    print('Recall Score:', recall_score(testLabelGold, testLabelPredicted, average=\"macro\"))\n",
        "    print('Precision Score:', precision_score(testLabelGold, testLabelPredicted, average=\"macro\"))\n",
        "    print('F1 Score:', f1_score(testLabelGold, testLabelPredicted, average=\"macro\"))\n",
        "    print('Accuracy:', accuracy_score(testLabelGold, testLabelPredicted))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOHPy5FFv7fF"
      },
      "outputs": [],
      "source": [
        "# Import Section\n",
        "import csv\n",
        "import sys\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics import *\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-large-uncased')\n",
        "\n",
        "# Set device to GPU if available, else use CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Move BERT model to GPU\n",
        "bert_model = bert_model.to(device)\n",
        "\n",
        "def get_bert_embeddings(texts, batch_size=8):\n",
        "    # Tokenize text\n",
        "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
        "\n",
        "    # Move inputs to device (GPU or CPU)\n",
        "    for key in inputs:\n",
        "        inputs[key] = inputs[key].to(device)\n",
        "\n",
        "    # Create DataLoader with the specified batch size\n",
        "    data_loader = torch.utils.data.DataLoader(list(range(len(texts))), batch_size=batch_size)\n",
        "\n",
        "    # List to store pooled embeddings\n",
        "    pooled_embeddings_list = []\n",
        "\n",
        "    # Iterate over the data loader\n",
        "    with torch.no_grad():\n",
        "        for batch_indices in data_loader:\n",
        "            # Get the batched input tensors using indices\n",
        "            batch_inputs = {key: inputs[key][batch_indices] for key in inputs}\n",
        "            # Pass the tokenized input through BERT model\n",
        "            outputs = bert_model(**batch_inputs)\n",
        "            # Obtain embeddings from BERT model\n",
        "            embeddings = outputs.last_hidden_state\n",
        "            # Perform pooling (e.g., mean pooling) to obtain fixed-size representation\n",
        "            pooled_embeddings = torch.mean(embeddings, dim=1)  # You can use other pooling strategies\n",
        "            # Move the pooled embeddings to the CPU and append to the list\n",
        "            pooled_embeddings_list.append(pooled_embeddings.cpu())\n",
        "\n",
        "    # Concatenate pooled embeddings from all batches to obtain the final embeddings\n",
        "    pooled_embeddings = torch.cat(pooled_embeddings_list, dim=0)\n",
        "\n",
        "    return pooled_embeddings\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    tweets = []\n",
        "    label = []\n",
        "\n",
        "    with open('/content/drive/MyDrive/Colab Notebooks/Thesis/train_en.txt', 'r') as f:\n",
        "        next(f) # skip headings\n",
        "        reader = csv.reader(f, dialect=\"excel-tab\")\n",
        "        for line in reader:\n",
        "            preProcessedTweetText = line[1]\n",
        "            #if (line[2]==\"1\"):\n",
        "            tweets.append(preProcessedTweetText)\n",
        "            if(line[3]==\"1\"):\n",
        "              #cnt=cnt+1\n",
        "              label.append(\"individual\")\n",
        "            elif(line[3]==\"0\"):\n",
        "              label.append(\"generic\")\n",
        "\n",
        "    f.close()\n",
        "\n",
        "    X_train = tweets\n",
        "    Y_train = np.array(label)\n",
        "\n",
        "    # Get BERT embeddings for training data\n",
        "    X_train_bert = get_bert_embeddings(X_train)\n",
        "\n",
        "    classifier = Pipeline([\n",
        "        ('clf', MLPClassifier())\n",
        "    ])\n",
        "\n",
        "\n",
        "    ## Train Classifier\n",
        "    classifier.fit(X_train_bert, Y_train)\n",
        "\n",
        "    ## Test Set Prediction Module\n",
        "    testTweets = []\n",
        "    testLabelGold = []\n",
        "\n",
        "    with open('/content/drive/MyDrive/Colab Notebooks/Thesis/test.txt', 'r') as f:\n",
        "        reader = csv.reader(f, dialect=\"excel-tab\")\n",
        "        for line in reader:\n",
        "            preProcessedTweetText = line[1]\n",
        "            testTweets.append(preProcessedTweetText)\n",
        "            if line[3] == \"1\":\n",
        "                testLabelGold.append(\"individual\")\n",
        "            else:\n",
        "                testLabelGold.append(\"generic\")\n",
        "    f.close()\n",
        "\n",
        "    print(\"Complete test data reading .....\")\n",
        "    X_test = testTweets\n",
        "\n",
        "    # Get BERT embeddings for test data\n",
        "    X_test_bert = get_bert_embeddings(X_test)\n",
        "\n",
        "    # Test data prediction\n",
        "    testLabelPredicted = classifier.predict(X_test_bert)\n",
        "\n",
        "    # Evaluation\n",
        "    results = confusion_matrix(testLabelGold, testLabelPredicted)\n",
        "\n",
        "    print('Confusion Matrix:')\n",
        "    print(results)\n",
        "\n",
        "    print('Recall Score:', recall_score(testLabelGold, testLabelPredicted, average=\"macro\"))\n",
        "    print('Precision Score:', precision_score(testLabelGold, testLabelPredicted, average=\"macro\"))\n",
        "    print('F1 Score:', f1_score(testLabelGold, testLabelPredicted, average=\"macro\"))\n",
        "    print('Accuracy:', accuracy_score(testLabelGold, testLabelPredicted))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6z8dZYowCmP"
      },
      "outputs": [],
      "source": [
        "# Import Section\n",
        "import csv\n",
        "import sys\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics import *\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-large-uncased')\n",
        "\n",
        "# Set device to GPU if available, else use CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Move BERT model to GPU\n",
        "bert_model = bert_model.to(device)\n",
        "\n",
        "def get_bert_embeddings(texts, batch_size=8):\n",
        "    # Tokenize text\n",
        "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
        "\n",
        "    # Move inputs to device (GPU or CPU)\n",
        "    for key in inputs:\n",
        "        inputs[key] = inputs[key].to(device)\n",
        "\n",
        "    # Create DataLoader with the specified batch size\n",
        "    data_loader = torch.utils.data.DataLoader(list(range(len(texts))), batch_size=batch_size)\n",
        "\n",
        "    # List to store pooled embeddings\n",
        "    pooled_embeddings_list = []\n",
        "\n",
        "    # Iterate over the data loader\n",
        "    with torch.no_grad():\n",
        "        for batch_indices in data_loader:\n",
        "            # Get the batched input tensors using indices\n",
        "            batch_inputs = {key: inputs[key][batch_indices] for key in inputs}\n",
        "            # Pass the tokenized input through BERT model\n",
        "            outputs = bert_model(**batch_inputs)\n",
        "            # Obtain embeddings from BERT model\n",
        "            embeddings = outputs.last_hidden_state\n",
        "            # Perform pooling (e.g., mean pooling) to obtain fixed-size representation\n",
        "            pooled_embeddings = torch.mean(embeddings, dim=1)  # You can use other pooling strategies\n",
        "            # Move the pooled embeddings to the CPU and append to the list\n",
        "            pooled_embeddings_list.append(pooled_embeddings.cpu())\n",
        "\n",
        "    # Concatenate pooled embeddings from all batches to obtain the final embeddings\n",
        "    pooled_embeddings = torch.cat(pooled_embeddings_list, dim=0)\n",
        "\n",
        "    return pooled_embeddings\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    tweets = []\n",
        "    label = []\n",
        "\n",
        "    with open('/content/drive/MyDrive/Colab Notebooks/Thesis/train_en.txt', 'r') as f:\n",
        "        next(f) # skip headings\n",
        "        reader = csv.reader(f, dialect=\"excel-tab\")\n",
        "        for line in reader:\n",
        "            preProcessedTweetText = line[1]\n",
        "            #if (line[2]==\"1\"):\n",
        "            tweets.append(preProcessedTweetText)\n",
        "            if(line[4]==\"1\"):\n",
        "              #cnt=cnt+1\n",
        "              label.append(\"aggressive\")\n",
        "            elif(line[4]==\"0\"):\n",
        "              label.append(\"notAggressive\")\n",
        "\n",
        "    f.close()\n",
        "\n",
        "    X_train = tweets\n",
        "    Y_train = np.array(label)\n",
        "\n",
        "    # Get BERT embeddings for training data\n",
        "    X_train_bert = get_bert_embeddings(X_train)\n",
        "\n",
        "    classifier = Pipeline([\n",
        "        ('clf', MLPClassifier())\n",
        "    ])\n",
        "\n",
        "    ## Train Classifier\n",
        "    classifier.fit(X_train_bert, Y_train)\n",
        "\n",
        "    ## Test Set Prediction Module\n",
        "    testTweets = []\n",
        "    testLabelGold = []\n",
        "\n",
        "    with open('/content/drive/MyDrive/Colab Notebooks/Thesis/test.txt', 'r') as f:\n",
        "        reader = csv.reader(f, dialect=\"excel-tab\")\n",
        "        for line in reader:\n",
        "            preProcessedTweetText = line[1]\n",
        "            testTweets.append(preProcessedTweetText)\n",
        "            if line[4] == \"1\":\n",
        "                testLabelGold.append(\"aggressive\")\n",
        "            else:\n",
        "                testLabelGold.append(\"notAggressive\")\n",
        "    f.close()\n",
        "\n",
        "    print(\"Complete test data reading .....\")\n",
        "    X_test = testTweets\n",
        "\n",
        "    # Get BERT embeddings for test data\n",
        "    X_test_bert = get_bert_embeddings(X_test)\n",
        "\n",
        "    # Test data prediction\n",
        "    testLabelPredicted = classifier.predict(X_test_bert)\n",
        "\n",
        "    # Evaluation\n",
        "    results = confusion_matrix(testLabelGold, testLabelPredicted)\n",
        "\n",
        "    print('Confusion Matrix:')\n",
        "    print(results)\n",
        "\n",
        "    print('Recall Score:', recall_score(testLabelGold, testLabelPredicted, average=\"macro\"))\n",
        "    print('Precision Score:', precision_score(testLabelGold, testLabelPredicted, average=\"macro\"))\n",
        "    print('F1 Score:', f1_score(testLabelGold, testLabelPredicted, average=\"macro\"))\n",
        "    print('Accuracy:', accuracy_score(testLabelGold, testLabelPredicted))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Final with word2vec"
      ],
      "metadata": {
        "id": "hsn6-I2y1pkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Section\n",
        "import csv\n",
        "import sys\n",
        "import joblib\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics import *\n",
        "import gensim.downloader as api\n",
        "import gradio as gr\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import preprocessing module\n",
        "sys.path.append(\"/content/drive/MyDrive/Colab Notebooks/Project2022\")\n",
        "import PreprocessingModule as cpm\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-large-uncased')\n",
        "\n",
        "# Set device to GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Move BERT model to GPU\n",
        "bert_model = bert_model.to(device)\n",
        "\n",
        "# Load pre-trained FastText word embeddings\n",
        "fasttext_model = api.load(\"fasttext-wiki-news-subwords-300\")\n",
        "\n",
        "# Load pre-trained Word2Vec embeddings\n",
        "word2vec_model = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "def fasttext_embeddings(texts):\n",
        "    embeddings_list = []\n",
        "    for text in texts:\n",
        "        text=cpm.preProcessingModule(text)\n",
        "        words = text.split()\n",
        "        # Compute the average FastText word embedding for the words in the text\n",
        "        embeddings = [fasttext_model[word] for word in words if word in fasttext_model]\n",
        "        if embeddings:\n",
        "            embeddings = np.mean(embeddings, axis=0)\n",
        "        else:\n",
        "            # Handle missing words by using zero vectors\n",
        "            embeddings = np.zeros(fasttext_model.vector_size)\n",
        "        embeddings_list.append(embeddings)\n",
        "    return torch.tensor(embeddings_list)\n",
        "\n",
        "def word2vec_embeddings(texts):\n",
        "    embeddings_list = []\n",
        "    for text in texts:\n",
        "        text=cpm.preProcessingModule(text)\n",
        "        words = text.split()\n",
        "        # Compute the average Word2Vec word embedding for the words in the text\n",
        "        embeddings = [word2vec_model[word] for word in words if word in word2vec_model]\n",
        "        if embeddings:\n",
        "            embeddings = np.mean(embeddings, axis=0)\n",
        "        else:\n",
        "            # Handle missing words by using zero vectors\n",
        "            embeddings = np.zeros(word2vec_model.vector_size)\n",
        "        embeddings_list.append(embeddings)\n",
        "    return torch.tensor(embeddings_list)\n",
        "\n",
        "def bert_embeddings(texts, batch_size=16):\n",
        "    # Tokenize text\n",
        "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
        "\n",
        "    # Move inputs to device (GPU or CPU)\n",
        "    for key in inputs:\n",
        "        inputs[key] = inputs[key].to(device)\n",
        "\n",
        "    # Create DataLoader\n",
        "    data_loader = torch.utils.data.DataLoader(list(range(len(texts))), batch_size=batch_size)\n",
        "\n",
        "    pooled_embeddings_list = []\n",
        "\n",
        "    # Iterate over the data loader\n",
        "    with torch.no_grad():\n",
        "        for batch_indices in data_loader:\n",
        "            batch_inputs = {key: inputs[key][batch_indices] for key in inputs}\n",
        "            # Pass the tokenized input through BERT model\n",
        "            outputs = bert_model(**batch_inputs)\n",
        "            # Obtain embeddings from BERT model\n",
        "            embeddings = outputs.last_hidden_state\n",
        "            # Perform mean pooling\n",
        "            pooled_embeddings = torch.mean(embeddings, dim=1)\n",
        "\n",
        "            pooled_embeddings_list.append(pooled_embeddings.cpu())\n",
        "\n",
        "    # Concatenate pooled embeddings from all batches\n",
        "    pooled_embeddings = torch.cat(pooled_embeddings_list, dim=0)\n",
        "\n",
        "    return pooled_embeddings\n",
        "\n",
        "def main():\n",
        "    tweets = []\n",
        "    label = []\n",
        "    label1 = []\n",
        "    label2 = []\n",
        "\n",
        "    with open('/content/drive/MyDrive/Colab Notebooks/Thesis/train_en.txt', 'r') as f:\n",
        "        next(f)  # skip headings\n",
        "        reader = csv.reader(f, dialect=\"excel-tab\")\n",
        "        for line in reader:\n",
        "            preProcessedTweetText = line[1]\n",
        "            tweets.append(preProcessedTweetText)\n",
        "            if line[2] == \"1\":\n",
        "                label.append(\"hateful\")\n",
        "            else:\n",
        "                label.append(\"notHateful\")\n",
        "\n",
        "            if line[3] == \"1\":\n",
        "                label1.append(\"individual\")\n",
        "            else:\n",
        "                label1.append(\"generic\")\n",
        "\n",
        "            if line[4] == \"1\":\n",
        "                label2.append(\"aggressive\")\n",
        "            else:\n",
        "                label2.append(\"notAggressive\")\n",
        "\n",
        "    f.close()\n",
        "\n",
        "    X_train = tweets\n",
        "    Y_train = np.array(label)\n",
        "    Y_train1 = np.array(label1)\n",
        "    Y_train2 = np.array(label2)\n",
        "\n",
        "    # Get BERT embeddings for training data\n",
        "    X_train_bert = bert_embeddings(X_train)\n",
        "\n",
        "    print(X_train_bert)\n",
        "\n",
        "    # Get FastText embeddings for training data\n",
        "    X_train_fasttext = fasttext_embeddings(X_train)\n",
        "    print(X_train_fasttext)\n",
        "\n",
        "    # Get Word2Vec embeddings for training data\n",
        "    X_train_word2vec = word2vec_embeddings(X_train)\n",
        "    print(X_train_word2vec)\n",
        "\n",
        "    # Combine BERT, FastText, and Word2Vec embeddings\n",
        "    X_train_combined = torch.cat((X_train_bert, X_train_fasttext, X_train_word2vec), dim=1)\n",
        "    print(X_train_combined)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_combined_scaled = scaler.fit_transform(X_train_combined)\n",
        "    print(X_train_combined_scaled)\n",
        "\n",
        "    classifier = Pipeline([\n",
        "        ('clf', MLPClassifier(hidden_layer_sizes=(10,), max_iter=32, alpha=1e-1, activation='relu',\n",
        "                              solver='adam', beta_1=0.44, beta_2=0.999,\n",
        "                              random_state=0, batch_size=128, epsilon=1e-5))\n",
        "    ])\n",
        "\n",
        "    classifier1 = Pipeline([\n",
        "        ('clf', MLPClassifier(hidden_layer_sizes=(10,), max_iter=32, alpha=1e-1, activation='relu',\n",
        "                              solver='adam', random_state=0, batch_size=128, epsilon=1e-5))\n",
        "    ])\n",
        "\n",
        "    classifier2 = Pipeline([\n",
        "        ('clf', MLPClassifier(hidden_layer_sizes=(10,), max_iter=32, alpha=1e-1, activation='relu',\n",
        "                              solver='adam', random_state=0, batch_size=128, epsilon=1e-5))\n",
        "    ])\n",
        "\n",
        "    # Train Classifier\n",
        "    classifier.fit(X_train_combined_scaled, Y_train)\n",
        "    classifier1.fit(X_train_combined_scaled, Y_train1)\n",
        "    classifier2.fit(X_train_combined_scaled, Y_train2)\n",
        "\n",
        "    ## Test Set Prediction Module\n",
        "    testTweets = []\n",
        "    testLabelGold = []\n",
        "    testLabelGold1 = []\n",
        "    testLabelGold2 = []\n",
        "\n",
        "    with open('/content/drive/MyDrive/Colab Notebooks/Thesis/test.txt', 'r') as f:\n",
        "        next(f)\n",
        "        reader = csv.reader(f, dialect=\"excel-tab\")\n",
        "        for line in reader:\n",
        "            preProcessedTweetText = line[1]\n",
        "            testTweets.append(preProcessedTweetText)\n",
        "            if line[2] == \"1\":\n",
        "                testLabelGold.append(\"hateful\")\n",
        "            else:\n",
        "                testLabelGold.append(\"notHateful\")\n",
        "\n",
        "            if line[3] == \"1\":\n",
        "                testLabelGold1.append(\"individual\")\n",
        "            else:\n",
        "                testLabelGold1.append(\"generic\")\n",
        "\n",
        "            if line[4] == \"1\":\n",
        "                testLabelGold2.append(\"aggressive\")\n",
        "            else:\n",
        "                testLabelGold2.append(\"notAggressive\")\n",
        "\n",
        "    f.close()\n",
        "\n",
        "    print(\"Complete test data reading .....\")\n",
        "    X_test = testTweets\n",
        "\n",
        "    # Get BERT embeddings for test data\n",
        "    X_test_bert = bert_embeddings(X_test)\n",
        "\n",
        "    # Get FastText embeddings for test data\n",
        "    X_test_fasttext = fasttext_embeddings(X_test)\n",
        "\n",
        "    # Get Word2Vec embeddings for test data\n",
        "    X_test_word2vec = word2vec_embeddings(X_test)\n",
        "\n",
        "    # Combine BERT, FastText, and Word2Vec embeddings\n",
        "    X_test_combined = torch.cat((X_test_bert, X_test_fasttext, X_test_word2vec), dim=1)\n",
        "\n",
        "    # Test data prediction\n",
        "    X_test_combined_scaled = scaler.transform(X_test_combined)\n",
        "\n",
        "    testLabelPredicted = classifier.predict(X_test_combined_scaled)\n",
        "    testLabelPredicted1 = classifier1.predict(X_test_combined_scaled)\n",
        "    testLabelPredicted2 = classifier2.predict(X_test_combined_scaled)\n",
        "\n",
        "    # Evaluation\n",
        "    class_names = ['hateful', 'notHateful']\n",
        "    print(\"Classification Report of HS:\")\n",
        "    print(classification_report(testLabelGold, testLabelPredicted, target_names=class_names))\n",
        "\n",
        "    results = confusion_matrix(testLabelGold, testLabelPredicted)\n",
        "    results1 = confusion_matrix(testLabelGold1, testLabelPredicted1)\n",
        "    results2 = confusion_matrix(testLabelGold2, testLabelPredicted2)\n",
        "\n",
        "    print('Confusion Matrix of HS:')\n",
        "    print(results)\n",
        "\n",
        "\n",
        "    print('Recall Score:', recall_score(testLabelGold, testLabelPredicted, average=\"macro\"))\n",
        "    print('Precision Score:', precision_score(testLabelGold, testLabelPredicted, average=\"macro\"))\n",
        "    print('F1 Score:', f1_score(testLabelGold, testLabelPredicted, average=\"macro\"))\n",
        "    print('Accuracy:', accuracy_score(testLabelGold, testLabelPredicted))\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print('Confusion Matrix of TR:')\n",
        "    print(results1)\n",
        "\n",
        "    print('Recall Score:', recall_score(testLabelGold1, testLabelPredicted1, average=\"macro\"))\n",
        "    print('Precision Score:', precision_score(testLabelGold1, testLabelPredicted1, average=\"macro\"))\n",
        "    print('F1 Score:', f1_score(testLabelGold1, testLabelPredicted1, average=\"macro\"))\n",
        "    print('Accuracy:', accuracy_score(testLabelGold1, testLabelPredicted1))\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print('Confusion Matrix of AG:')\n",
        "    print(results2)\n",
        "\n",
        "    print('Recall Score:', recall_score(testLabelGold2, testLabelPredicted2, average=\"macro\"))\n",
        "    print('Precision Score:', precision_score(testLabelGold2, testLabelPredicted2, average=\"macro\"))\n",
        "    print('F1 Score:', f1_score(testLabelGold2, testLabelPredicted2, average=\"macro\"))\n",
        "    print('Accuracy:', accuracy_score(testLabelGold2, testLabelPredicted2))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0V2TXOeM10Xg",
        "outputId": "91438c42-bf28-4559-c92b-cf1110d0d0a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "[==================================================] 100.0% 958.5/958.4MB downloaded\n",
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
            "tensor([[-0.2812, -0.0708, -0.1025,  ..., -0.1670, -0.1111,  0.2747],\n",
            "        [ 0.0835,  0.1322, -0.2195,  ..., -0.1943, -0.1703,  0.2581],\n",
            "        [-0.1818, -0.0917, -0.2757,  ..., -0.2642, -0.0349,  0.1817],\n",
            "        ...,\n",
            "        [ 0.0187, -0.1545, -0.5340,  ..., -0.1533,  0.0364,  0.3309],\n",
            "        [ 0.2461, -0.1326, -0.1654,  ...,  0.1752, -0.1052,  0.0389],\n",
            "        [ 0.0791, -0.2272, -0.4102,  ..., -0.2416,  0.2388,  0.0708]])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-6e7f71358e5c>:54: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(embeddings_list)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.0109, -0.0402,  0.0409,  ...,  0.0037,  0.0361,  0.0087],\n",
            "        [ 0.0157, -0.0077,  0.0299,  ...,  0.0172,  0.0011, -0.0064],\n",
            "        [ 0.0061,  0.0128,  0.0106,  ..., -0.0036, -0.0278, -0.0263],\n",
            "        ...,\n",
            "        [-0.0089,  0.0004,  0.0138,  ...,  0.0154,  0.0109, -0.0167],\n",
            "        [-0.0376,  0.0211,  0.0359,  ...,  0.0070,  0.0204, -0.0069],\n",
            "        [ 0.0092,  0.0294,  0.0310,  ..., -0.0146,  0.0085,  0.0034]],\n",
            "       dtype=torch.float64)\n",
            "tensor([[ 0.1303,  0.1192, -0.0189,  ...,  0.0352,  0.0101,  0.0364],\n",
            "        [ 0.0725,  0.0864,  0.0059,  ..., -0.0411,  0.0552,  0.0306],\n",
            "        [ 0.0536,  0.0714,  0.0562,  ..., -0.1268,  0.0140, -0.0688],\n",
            "        ...,\n",
            "        [ 0.0729, -0.0656,  0.0439,  ..., -0.0615, -0.0042, -0.0122],\n",
            "        [ 0.1937, -0.0781, -0.0260,  ...,  0.0129,  0.0153,  0.1025],\n",
            "        [ 0.0927, -0.0764,  0.0002,  ..., -0.1205,  0.0347, -0.0571]],\n",
            "       dtype=torch.float64)\n",
            "tensor([[-0.2812, -0.0708, -0.1025,  ...,  0.0352,  0.0101,  0.0364],\n",
            "        [ 0.0835,  0.1322, -0.2195,  ..., -0.0411,  0.0552,  0.0306],\n",
            "        [-0.1818, -0.0917, -0.2757,  ..., -0.1268,  0.0140, -0.0688],\n",
            "        ...,\n",
            "        [ 0.0187, -0.1545, -0.5340,  ..., -0.0615, -0.0042, -0.0122],\n",
            "        [ 0.2461, -0.1326, -0.1654,  ...,  0.0129,  0.0153,  0.1025],\n",
            "        [ 0.0791, -0.2272, -0.4102,  ..., -0.1205,  0.0347, -0.0571]],\n",
            "       dtype=torch.float64)\n",
            "[[-2.04357316 -0.46958909  0.75959068 ...  1.81783681  0.31640964\n",
            "   0.16526641]\n",
            " [ 0.2153102   0.83294192  0.06949343 ...  0.60438861  0.89182389\n",
            "   0.06830919]\n",
            " [-1.42779279 -0.60365159 -0.26200281 ... -0.75999139  0.3657642\n",
            "  -1.57068752]\n",
            " ...\n",
            " [-0.18610593 -1.00682045 -1.78602911 ...  0.27937933  0.13357903\n",
            "  -0.63743162]\n",
            " [ 1.22235808 -0.86625009  0.38828773 ...  1.4630074   0.38272764\n",
            "   1.25536794]\n",
            " [ 0.18798468 -1.47358596 -1.0553929  ... -0.65845811  0.62983028\n",
            "  -1.37893773]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (32) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (32) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (32) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Complete test data reading .....\n",
            "Classification Report of HS:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     hateful       0.50      0.85      0.63      1252\n",
            "  notHateful       0.78      0.37      0.50      1719\n",
            "\n",
            "    accuracy                           0.57      2971\n",
            "   macro avg       0.64      0.61      0.57      2971\n",
            "weighted avg       0.66      0.57      0.56      2971\n",
            "\n",
            "Confusion Matrix of HS:\n",
            "[[1070  182]\n",
            " [1081  638]]\n",
            "Recall Score: 0.6128893014922489\n",
            "Precision Score: 0.6377459151160549\n",
            "F1 Score: 0.5657084769977421\n",
            "Accuracy: 0.574890609222484\n",
            "\n",
            "\n",
            "Confusion Matrix of TR:\n",
            "[[1953  496]\n",
            " [ 147  375]]\n",
            "Recall Score: 0.7579295795140404\n",
            "Precision Score: 0.6802698048220437\n",
            "F1 Score: 0.6985282850518018\n",
            "Accuracy: 0.7835745540222148\n",
            "\n",
            "\n",
            "Confusion Matrix of AG:\n",
            "[[ 270  320]\n",
            " [ 556 1825]]\n",
            "Recall Score: 0.6120558944753308\n",
            "Precision Score: 0.5888461820665211\n",
            "F1 Score: 0.5939037725533078\n",
            "Accuracy: 0.705149781218445\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b23FE34A6zEy"
      },
      "source": [
        "# Final Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "78859695a1eb4eb99ba00b9f92d88aa3",
            "28a45ab5564b432baf3f0a002b772c6e",
            "8c06ae1e4a0e4c138a873edf8795b224",
            "a5eff5777cf240d4af862cdc37b476c1",
            "4d0492bacdeb4176b2a479b7a82dd419",
            "62959bc6198a45198b504ad3e21dd0c6",
            "5e5a84277a8e41b6bb45f86a6a414d68",
            "3d2a0383a4044b3eb6db2c7599d9f652",
            "157d3e5c096b40369fec4c81f7f72c2d",
            "d1aa5a3c9d82428bb882fc5ad79116a2",
            "9898ffa65f2944b5a439060b406ddca7",
            "84dad553b8ab4c8b97c62efeda488080",
            "23abfc1bfec7436b983c6a4edb25d865",
            "c2add516222a428aa4a698db669ae7f7",
            "d8ed91be0f1a405599a3a637f24af726",
            "8a2c174eb6614ed3afa4ca6b899dec50",
            "0ca28968107348c4ad8a4f3cb3f6bd9b",
            "7fe564bf8ee149e4bcfa594686aecf49",
            "e11d4242d5d74490837e3079e3826c35",
            "6da507ecc33a4b1e9d0b3b3ea8faa511",
            "fe8c458b085c4e42b5f7e71bf1eeb562",
            "bcc8c91e671c4a898665772a01fc6a6e",
            "c008e6b848b449f6b9f2b166a9a510e8",
            "a42e301e776b41009f2f5e75f37d85aa",
            "f9b769aa11b7444993fe79a199ba6bcf",
            "fe54535654424fb386e8a679c990be26",
            "f9d2dbbd68ed4ebcbb76a923e5929c43",
            "a5bebbb022d14bf785e1757912e8b027",
            "1ba07fed045847288e9c55e4dcde8cf3",
            "ab534ac42b574d69a4c6451a07783d1d",
            "61e8bdb5fab843adb0351042e8503338",
            "d1e4b6e38577401096a03c7f0eac467c",
            "013e8f736de9432c8155b09ea3304dd2",
            "12b1960c16a04901be21e9f053076ecb",
            "0047571ab5cb4d578396cca13cb22953",
            "8a191bda980b47998b35eb0287601df7",
            "c793c09d6e7244abaad69b4d575d33b7",
            "efaff630771a4901809b55aefdd22ca3",
            "2d26c271ca1b469c866f7cfd534abaf1",
            "78a5411860554259bc073d33053072db",
            "59675dad38b341b7aa329a5417548d65",
            "a1160d6f0a4a4afcb0fc5292f1f656dd",
            "f66be7c7dbec41aba9d9d240409d0a56",
            "26c1cca17e2440b3a9f2771e33231f10",
            "5fd969b6abb344829f808b272c3379bf",
            "c911fc4cd9ee4d5abe4d25dcfe9922d6",
            "6ac9466d660743d58e756516e4fb24ec",
            "56f3e75190f4474b92729221b25599d7",
            "088e16bac72f4ff888c55afebd5afc07",
            "ea771a1907484dfa977f3d8add4fa6e3",
            "c559b10c408c4717b270bf4a1fc2d3fe",
            "c95d18e786d24e1ba555efa2b8cb9e66",
            "48c7dbab5b30441eab8b8222b32323fb",
            "d139d59447bc468bb6bed86adc54e583",
            "c2a21be083b94280ba181c371cad5583"
          ]
        },
        "id": "2BXCJrOB7V82",
        "outputId": "b67e4777-92ad-4aea-bcc7-b1070ed649e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word statistics files not found!\n",
            "Downloading... done!\n",
            "Unpacking... done!\n",
            "Reading twitter - 1grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/twitter/counts_1grams.txt\n",
            "Reading twitter - 2grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/twitter/counts_2grams.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
            "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "78859695a1eb4eb99ba00b9f92d88aa3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "84dad553b8ab4c8b97c62efeda488080"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c008e6b848b449f6b9f2b166a9a510e8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "12b1960c16a04901be21e9f053076ecb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5fd969b6abb344829f808b272c3379bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "[==================================================] 100.0% 958.5/958.4MB downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-fc368e4f364b>:53: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(embeddings_list)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (32) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (32) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (32) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Complete test data reading .....\n",
            "Classification Report of HS:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     hateful       0.55      0.77      0.64      1252\n",
            "  notHateful       0.76      0.53      0.63      1719\n",
            "\n",
            "    accuracy                           0.63      2971\n",
            "   macro avg       0.65      0.65      0.63      2971\n",
            "weighted avg       0.67      0.63      0.63      2971\n",
            "\n",
            "Confusion Matrix of HS:\n",
            "[[969 283]\n",
            " [808 911]]\n",
            "Recall Score: 0.6519604699961157\n",
            "Precision Score: 0.6541413218785731\n",
            "F1 Score: 0.6326435712361465\n",
            "Accuracy: 0.6327835745540222\n",
            "\n",
            "\n",
            "Confusion Matrix of TR:\n",
            "[[2042  407]\n",
            " [ 140  382]]\n",
            "Recall Score: 0.7828052422679364\n",
            "Precision Score: 0.7099979205366177\n",
            "F1 Score: 0.732322106798264\n",
            "Accuracy: 0.8158869067653989\n",
            "\n",
            "\n",
            "Confusion Matrix of AG:\n",
            "[[ 247  343]\n",
            " [ 352 2029]]\n",
            "Recall Score: 0.6354035122687377\n",
            "Precision Score: 0.6338751066279662\n",
            "F1 Score: 0.6346258757029556\n",
            "Accuracy: 0.7660720296196567\n"
          ]
        }
      ],
      "source": [
        "# Import Section\n",
        "import csv\n",
        "import sys\n",
        "import joblib\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics import *\n",
        "import gensim.downloader as api\n",
        "import gradio as gr\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import preprocessing module\n",
        "sys.path.append(\"/content/drive/MyDrive/Colab Notebooks/Project2022\")\n",
        "import PreprocessingModule as cpm\n",
        "import PreprocessingModule1 as cpm1\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-large-uncased')\n",
        "\n",
        "# Set device to GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Move BERT model to GPU\n",
        "bert_model = bert_model.to(device)\n",
        "\n",
        "# Load pre-trained FastText word embeddings\n",
        "fasttext_model = api.load(\"fasttext-wiki-news-subwords-300\")\n",
        "\n",
        "def fasttext_embeddings(texts):\n",
        "    embeddings_list = []\n",
        "    for text in texts:\n",
        "        text=cpm.preProcessingModule(text)\n",
        "        #text=text.lower()\n",
        "        words = text.split()\n",
        "        # Compute the average FastText word embedding for the words in the text\n",
        "        embeddings = [fasttext_model[word] for word in words if word in fasttext_model]\n",
        "        if embeddings:\n",
        "            embeddings = np.mean(embeddings, axis=0)\n",
        "        else:\n",
        "            # Handle missing words by using zero vectors\n",
        "            embeddings = np.zeros(fasttext_model.vector_size)\n",
        "        embeddings_list.append(embeddings)\n",
        "    return torch.tensor(embeddings_list)\n",
        "\n",
        "def bert_embeddings(texts, batch_size=16):\n",
        "\n",
        "    preprocessed = []\n",
        "    for text in texts:\n",
        "        text=cpm1.preProcessingModule(text)\n",
        "        preprocessed.append(text)\n",
        "\n",
        "    texts = preprocessed\n",
        "    # Tokenize text\n",
        "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
        "\n",
        "    # Move inputs to device (GPU or CPU)\n",
        "    for key in inputs:\n",
        "        inputs[key] = inputs[key].to(device)\n",
        "\n",
        "    # Create DataLoader\n",
        "    data_loader = torch.utils.data.DataLoader(list(range(len(texts))), batch_size=batch_size)\n",
        "\n",
        "    pooled_embeddings_list = []\n",
        "\n",
        "    # Iterate over the data loader\n",
        "    with torch.no_grad():\n",
        "        for batch_indices in data_loader:\n",
        "\n",
        "            batch_inputs = {key: inputs[key][batch_indices] for key in inputs}\n",
        "            # Pass the tokenized input through BERT model\n",
        "            outputs = bert_model(**batch_inputs)\n",
        "            # Obtain embeddings from BERT model\n",
        "            embeddings = outputs.last_hidden_state\n",
        "            # Perform mean pooling\n",
        "            pooled_embeddings = torch.mean(embeddings, dim=1)\n",
        "\n",
        "            pooled_embeddings_list.append(pooled_embeddings.cpu())\n",
        "\n",
        "    # Concatenate pooled embeddings from all batches\n",
        "    pooled_embeddings = torch.cat(pooled_embeddings_list, dim=0)\n",
        "\n",
        "    return pooled_embeddings\n",
        "\n",
        "def main():\n",
        "    tweets = []\n",
        "    label = []\n",
        "    label1= []\n",
        "    label2= []\n",
        "\n",
        "    with open('/content/drive/MyDrive/Colab Notebooks/Thesis/train_en.txt', 'r') as f:\n",
        "        next(f) # skip headings\n",
        "        reader = csv.reader(f, dialect=\"excel-tab\")\n",
        "        for line in reader:\n",
        "            preProcessedTweetText = line[1]\n",
        "            tweets.append(preProcessedTweetText)\n",
        "            if line[2] == \"1\":\n",
        "                label.append(\"hateful\")\n",
        "            else:\n",
        "                label.append(\"notHateful\")\n",
        "\n",
        "            if(line[3]==\"1\"):\n",
        "\n",
        "              label1.append(\"individual\")\n",
        "            else:\n",
        "              label1.append(\"generic\")\n",
        "\n",
        "            if(line[4]==\"1\"):\n",
        "\n",
        "              label2.append(\"aggressive\")\n",
        "            else:\n",
        "              label2.append(\"notAggressive\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    f.close()\n",
        "\n",
        "    X_train = tweets\n",
        "    Y_train = np.array(label)\n",
        "    Y_train1 = np.array(label1)\n",
        "    Y_train2 = np.array(label2)\n",
        "\n",
        "    # Get BERT embeddings for training data\n",
        "    X_train_bert = bert_embeddings(X_train)\n",
        "\n",
        "    ## Get FastText embeddings for training data\n",
        "    X_train_fasttext = fasttext_embeddings(X_train)\n",
        "\n",
        "    ## Combine FastText and BERT embeddings\n",
        "    X_train_combined = torch.cat((X_train_bert, X_train_fasttext), dim=1)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_combined_scaled = scaler.fit_transform(X_train_combined)\n",
        "\n",
        "    classifier = Pipeline([\n",
        "        ('clf', MLPClassifier(hidden_layer_sizes=(10,), max_iter=32, alpha=1e-1, activation='relu',\n",
        "                              solver='adam', beta_1=0.44, beta_2=0.999,\n",
        "                              # validation_fraction=0.1,\n",
        "                              random_state=0, batch_size= 128 , epsilon=1e-5))\n",
        "    ])\n",
        "\n",
        "    classifier1 = Pipeline([\n",
        "        ('clf', MLPClassifier(hidden_layer_sizes=(10,), max_iter=32, alpha=1e-1, activation='relu',\n",
        "                              solver='adam', #beta_1=0.44, beta_2=0.999,\n",
        "                              random_state=0, batch_size= 128 , epsilon=1e-5))\n",
        "    ])\n",
        "\n",
        "    classifier2 = Pipeline([\n",
        "        ('clf', MLPClassifier(hidden_layer_sizes=(10,), max_iter=32, alpha=1e-1, activation='relu',\n",
        "                              solver='adam', #beta_1=0.44, beta_2=0.999,\n",
        "                              random_state=0, batch_size= 128 , epsilon=1e-5))\n",
        "    ])\n",
        "\n",
        "    ## Train Classifier\n",
        "\n",
        "    classifier.fit(X_train_combined, Y_train)\n",
        "\n",
        "    classifier1.fit(X_train_combined, Y_train1)\n",
        "\n",
        "    classifier2.fit(X_train_combined, Y_train2)\n",
        "\n",
        "    #model_filename = 'trained_classifier_model.pkl'\n",
        "    #joblib.dump(classifier, model_filename)\n",
        "\n",
        "\n",
        "\n",
        "    ## Test Set Prediction Module\n",
        "    testTweets = []\n",
        "    testLabelGold = []\n",
        "    testLabelGold1= []\n",
        "    testLabelGold2= []\n",
        "\n",
        "    with open('/content/drive/MyDrive/Colab Notebooks/Thesis/test.txt', 'r') as f:\n",
        "        next(f)\n",
        "        reader = csv.reader(f, dialect=\"excel-tab\")\n",
        "        for line in reader:\n",
        "            preProcessedTweetText = line[1]\n",
        "            testTweets.append(preProcessedTweetText)\n",
        "            if line[2] == \"1\":\n",
        "                testLabelGold.append(\"hateful\")\n",
        "            else:\n",
        "                testLabelGold.append(\"notHateful\")\n",
        "\n",
        "            if(line[3] == \"1\"):\n",
        "                testLabelGold1.append(\"individual\")\n",
        "            else:\n",
        "                testLabelGold1.append(\"generic\")\n",
        "\n",
        "            if(line[4] == \"1\"):\n",
        "                testLabelGold2.append(\"aggressive\")\n",
        "            else:\n",
        "                testLabelGold2.append(\"notAggressive\")\n",
        "\n",
        "\n",
        "    f.close()\n",
        "\n",
        "    print(\"Complete test data reading .....\")\n",
        "    X_test = testTweets\n",
        "\n",
        "    # Get BERT embeddings for test data\n",
        "    X_test_bert = bert_embeddings(X_test)\n",
        "\n",
        "    ## Get FastText embeddings for test data\n",
        "    X_test_fasttext = fasttext_embeddings(X_test)\n",
        "\n",
        "    ## Combine FastText and BERT embeddings\n",
        "    #X_test_combined = np.concatenate((X_test_bert.cpu().numpy(), X_test_fasttext), axis=1)\n",
        "\n",
        "    X_test_combined = torch.cat((X_test_bert, X_test_fasttext), dim=1)\n",
        "\n",
        "    # Test data prediction\n",
        "\n",
        "    X_test_combined_scaled = scaler.fit_transform(X_test_combined)\n",
        "\n",
        "    testLabelPredicted = classifier.predict(X_test_combined)\n",
        "\n",
        "    testLabelPredicted1 = classifier1.predict(X_test_combined)\n",
        "\n",
        "    testLabelPredicted2 = classifier2.predict(X_test_combined)\n",
        "\n",
        "    # Evaluation\n",
        "    class_names = ['hateful', 'notHateful']\n",
        "    print(\"Classification Report of HS:\")\n",
        "    print(classification_report(testLabelGold, testLabelPredicted, target_names=class_names))\n",
        "\n",
        "    results = confusion_matrix(testLabelGold, testLabelPredicted)\n",
        "\n",
        "    results1 = confusion_matrix(testLabelGold1, testLabelPredicted1)\n",
        "\n",
        "    results2 = confusion_matrix(testLabelGold2, testLabelPredicted2)\n",
        "\n",
        "    # with open('/content/drive/MyDrive/Colab Notebooks/Thesis/test.txt', 'r') as f1:\n",
        "    #   with open('/content/drive/MyDrive/Colab Notebooks/Thesis/test1.txt', 'w') as file1:\n",
        "    #     next(f1)\n",
        "    #     file1.write(\"id\"+\"\\t\"+\"text\"+\"\\t\"+\"HS\"+\"\\t\"+\"TR\"+\"\\t\"+\"AG\"+\"\\n\")\n",
        "    #     reader=csv.reader(f1, dialect=\"excel-tab\")\n",
        "    #     cnt=0\n",
        "    #     for line in reader:\n",
        "    #       if(testLabelPredicted[cnt]=='hateful'):\n",
        "    #         k=1\n",
        "    #       else:\n",
        "    #         k=0\n",
        "    #       file1.write(line[0]+\"\\t\"+line[1]+\"\\t\"+str(k)+\"\\t\"+line[3]+\"\\t\"+line[4]+\"\\n\")\n",
        "    #       cnt=cnt+1\n",
        "    #     print(cnt)\n",
        "    # f1.close()\n",
        "    # file1.close()\n",
        "\n",
        "\n",
        "    print('Confusion Matrix of HS:')\n",
        "    print(results)\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(results, annot=True, fmt='d', cmap=\"Blues\", xticklabels=['Hateful', 'Non-Hateful'],\n",
        "                yticklabels=['Hateful', 'Non-Hateful'])\n",
        "    plt.savefig('/content/drive/MyDrive/Colab Notebooks/Thesis/confusion_matrix_hs.pdf', format='pdf')\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "    print('Recall Score:', recall_score(testLabelGold, testLabelPredicted, average=\"macro\"))\n",
        "    print('Precision Score:', precision_score(testLabelGold, testLabelPredicted, average=\"macro\"))\n",
        "    print('F1 Score:', f1_score(testLabelGold, testLabelPredicted, average=\"macro\"))\n",
        "    print('Accuracy:', accuracy_score(testLabelGold, testLabelPredicted))\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print('Confusion Matrix of TR:')\n",
        "    print(results1)\n",
        "\n",
        "    print('Recall Score:', recall_score(testLabelGold1, testLabelPredicted1, average=\"macro\"))\n",
        "    print('Precision Score:', precision_score(testLabelGold1, testLabelPredicted1, average=\"macro\"))\n",
        "    print('F1 Score:', f1_score(testLabelGold1, testLabelPredicted1, average=\"macro\"))\n",
        "    print('Accuracy:', accuracy_score(testLabelGold1, testLabelPredicted1))\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print('Confusion Matrix of AG:')\n",
        "    print(results2)\n",
        "\n",
        "    print('Recall Score:', recall_score(testLabelGold2, testLabelPredicted2, average=\"macro\"))\n",
        "    print('Precision Score:', precision_score(testLabelGold2, testLabelPredicted2, average=\"macro\"))\n",
        "    print('F1 Score:', f1_score(testLabelGold2, testLabelPredicted2, average=\"macro\"))\n",
        "    print('Accuracy:', accuracy_score(testLabelGold2, testLabelPredicted2))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final with glove"
      ],
      "metadata": {
        "id": "zGWF8lgA4lQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Section\n",
        "import csv\n",
        "import sys\n",
        "import joblib\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics import *\n",
        "import gensim.downloader as api\n",
        "import gradio as gr\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import preprocessing module\n",
        "sys.path.append(\"/content/drive/MyDrive/Colab Notebooks/Project2022\")\n",
        "import PreprocessingModule as cpm\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-large-uncased')\n",
        "\n",
        "# Set device to GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Move BERT model to GPU\n",
        "bert_model = bert_model.to(device)\n",
        "\n",
        "# Load pre-trained FastText word embeddings\n",
        "fasttext_model = api.load(\"fasttext-wiki-news-subwords-300\")\n",
        "\n",
        "# Load pre-trained Glove embeddings\n",
        "glove_twitter_model = api.load(\"glove-twitter-200\")\n",
        "\n",
        "def fasttext_embeddings(texts):\n",
        "    embeddings_list = []\n",
        "    for text in texts:\n",
        "        text=cpm.preProcessingModule(text)\n",
        "        words = text.split()\n",
        "        # Compute the average FastText word embedding for the words in the text\n",
        "        embeddings = [fasttext_model[word] for word in words if word in fasttext_model]\n",
        "        if embeddings:\n",
        "            embeddings = np.mean(embeddings, axis=0)\n",
        "        else:\n",
        "            # Handle missing words by using zero vectors\n",
        "            embeddings = np.zeros(fasttext_model.vector_size)\n",
        "        embeddings_list.append(embeddings)\n",
        "    fasttext_tensor = torch.tensor(embeddings_list)\n",
        "    print(\"FastText embeddings:\\n\", fasttext_tensor)\n",
        "    return fasttext_tensor\n",
        "\n",
        "# GloVe embeddings\n",
        "def glove_embeddings(texts):\n",
        "    embeddings_list = []\n",
        "    for text in texts:\n",
        "        text = cpm.preProcessingModule(text)\n",
        "        words = text.split()\n",
        "        embeddings = [glove_twitter_model[word] for word in words if word in glove_twitter_model]\n",
        "        if embeddings:\n",
        "            embeddings = np.mean(embeddings, axis=0)\n",
        "        else:\n",
        "            embeddings = np.zeros(glove_twitter_model.vector_size)\n",
        "        embeddings_list.append(embeddings)\n",
        "    glove_tensor = torch.tensor(embeddings_list)\n",
        "    print(\"GloVe embeddings:\\n\", glove_tensor)\n",
        "    return glove_tensor\n",
        "\n",
        "\n",
        "def bert_embeddings(texts, batch_size=16):\n",
        "    # Tokenize text\n",
        "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
        "\n",
        "    # Move inputs to device (GPU or CPU)\n",
        "    for key in inputs:\n",
        "        inputs[key] = inputs[key].to(device)\n",
        "\n",
        "    # Create DataLoader\n",
        "    data_loader = torch.utils.data.DataLoader(list(range(len(texts))), batch_size=batch_size)\n",
        "\n",
        "    pooled_embeddings_list = []\n",
        "\n",
        "    # Iterate over the data loader\n",
        "    with torch.no_grad():\n",
        "        for batch_indices in data_loader:\n",
        "            batch_inputs = {key: inputs[key][batch_indices] for key in inputs}\n",
        "            # Pass the tokenized input through BERT model\n",
        "            outputs = bert_model(**batch_inputs)\n",
        "            # Obtain embeddings from BERT model\n",
        "            embeddings = outputs.last_hidden_state\n",
        "            # Perform mean pooling\n",
        "            pooled_embeddings = torch.mean(embeddings, dim=1)\n",
        "\n",
        "            pooled_embeddings_list.append(pooled_embeddings.cpu())\n",
        "\n",
        "    # Concatenate pooled embeddings from all batches\n",
        "    bert_tensor = torch.cat(pooled_embeddings_list, dim=0)\n",
        "    print(\"BERT embeddings:\\n\", bert_tensor)\n",
        "    return bert_tensor\n",
        "\n",
        "def main():\n",
        "    tweets = []\n",
        "    label = []\n",
        "    label1 = []\n",
        "    label2 = []\n",
        "\n",
        "    with open('/content/drive/MyDrive/Colab Notebooks/Thesis/train_en.txt', 'r') as f:\n",
        "        next(f)  # skip headings\n",
        "        reader = csv.reader(f, dialect=\"excel-tab\")\n",
        "        for line in reader:\n",
        "            preProcessedTweetText = line[1]\n",
        "            tweets.append(preProcessedTweetText)\n",
        "            if line[2] == \"1\":\n",
        "                label.append(\"hateful\")\n",
        "            else:\n",
        "                label.append(\"notHateful\")\n",
        "\n",
        "            if line[3] == \"1\":\n",
        "                label1.append(\"individual\")\n",
        "            else:\n",
        "                label1.append(\"generic\")\n",
        "\n",
        "            if line[4] == \"1\":\n",
        "                label2.append(\"aggressive\")\n",
        "            else:\n",
        "                label2.append(\"notAggressive\")\n",
        "\n",
        "    f.close()\n",
        "\n",
        "    X_train = tweets\n",
        "    Y_train = np.array(label)\n",
        "    Y_train1 = np.array(label1)\n",
        "    Y_train2 = np.array(label2)\n",
        "\n",
        "     # Get BERT embeddings for training data\n",
        "    X_train_bert = bert_embeddings(X_train)\n",
        "\n",
        "    # Get FastText embeddings for training data\n",
        "    X_train_fasttext = fasttext_embeddings(X_train)\n",
        "\n",
        "    # Get GloVe embeddings for training data\n",
        "    X_train_glove = glove_embeddings(X_train)\n",
        "\n",
        "    # Combine BERT, FastText, and GloVe embeddings\n",
        "    X_train_combined = torch.cat((X_train_bert, X_train_fasttext, X_train_glove), dim=1)\n",
        "    print(\"Combined embeddings:\\n\", X_train_combined)\n",
        "\n",
        "    # Scale combined embeddings\n",
        "    scaler = StandardScaler()\n",
        "    X_train_combined_scaled = scaler.fit_transform(X_train_combined)\n",
        "    print(\"Scaled combined embeddings:\\n\", X_train_combined_scaled)\n",
        "\n",
        "    classifier = Pipeline([\n",
        "        ('clf', MLPClassifier(hidden_layer_sizes=(10,), max_iter=32, alpha=1e-1, activation='relu',\n",
        "                              solver='adam', beta_1=0.44, beta_2=0.999,\n",
        "                              random_state=0, batch_size=128, epsilon=1e-5))\n",
        "    ])\n",
        "\n",
        "    classifier1 = Pipeline([\n",
        "        ('clf', MLPClassifier(hidden_layer_sizes=(10,), max_iter=32, alpha=1e-1, activation='relu',\n",
        "                              solver='adam', random_state=0, batch_size=128, epsilon=1e-5))\n",
        "    ])\n",
        "\n",
        "    classifier2 = Pipeline([\n",
        "        ('clf', MLPClassifier(hidden_layer_sizes=(10,), max_iter=32, alpha=1e-1, activation='relu',\n",
        "                              solver='adam', random_state=0, batch_size=128, epsilon=1e-5))\n",
        "    ])\n",
        "\n",
        "    # Train Classifier\n",
        "    classifier.fit(X_train_combined_scaled, Y_train)\n",
        "    classifier1.fit(X_train_combined_scaled, Y_train1)\n",
        "    classifier2.fit(X_train_combined_scaled, Y_train2)\n",
        "\n",
        "    ## Test Set Prediction Module\n",
        "    testTweets = []\n",
        "    testLabelGold = []\n",
        "    testLabelGold1 = []\n",
        "    testLabelGold2 = []\n",
        "\n",
        "    with open('/content/drive/MyDrive/Colab Notebooks/Thesis/test.txt', 'r') as f:\n",
        "        next(f)\n",
        "        reader = csv.reader(f, dialect=\"excel-tab\")\n",
        "        for line in reader:\n",
        "            preProcessedTweetText = line[1]\n",
        "            testTweets.append(preProcessedTweetText)\n",
        "            if line[2] == \"1\":\n",
        "                testLabelGold.append(\"hateful\")\n",
        "            else:\n",
        "                testLabelGold.append(\"notHateful\")\n",
        "\n",
        "            if line[3] == \"1\":\n",
        "                testLabelGold1.append(\"individual\")\n",
        "            else:\n",
        "                testLabelGold1.append(\"generic\")\n",
        "\n",
        "            if line[4] == \"1\":\n",
        "                testLabelGold2.append(\"aggressive\")\n",
        "            else:\n",
        "                testLabelGold2.append(\"notAggressive\")\n",
        "\n",
        "    f.close()\n",
        "\n",
        "    print(\"Complete test data reading .....\")\n",
        "    X_test = testTweets\n",
        "\n",
        "    # Get BERT embeddings for test data\n",
        "    X_test_bert = bert_embeddings(X_test)\n",
        "\n",
        "    # Get FastText embeddings for test data\n",
        "    X_test_fasttext = fasttext_embeddings(X_test)\n",
        "\n",
        "    # Get GloVe embeddings for test data\n",
        "    X_test_glove = glove_embeddings(X_test)\n",
        "\n",
        "    # Combine BERT, FastText, and GloVe embeddings\n",
        "    X_test_combined = torch.cat((X_test_bert, X_test_fasttext, X_test_glove), dim=1)\n",
        "    print(\"Combined test embeddings:\\n\", X_test_combined)\n",
        "\n",
        "    # Scale combined test embeddings\n",
        "    X_test_combined_scaled = scaler.transform(X_test_combined)\n",
        "    print(\"Scaled combined test embeddings:\\n\", X_test_combined_scaled)\n",
        "\n",
        "    # Predict on the test set\n",
        "    testLabelPredicted = classifier.predict(X_test_combined_scaled)\n",
        "    testLabelPredicted1 = classifier1.predict(X_test_combined_scaled)\n",
        "    testLabelPredicted2 = classifier2.predict(X_test_combined_scaled)\n",
        "\n",
        "    # Evaluation\n",
        "    class_names = ['hateful', 'notHateful']\n",
        "    print(\"Classification Report of HS:\")\n",
        "    print(classification_report(testLabelGold, testLabelPredicted, target_names=class_names))\n",
        "\n",
        "    results = confusion_matrix(testLabelGold, testLabelPredicted)\n",
        "    results1 = confusion_matrix(testLabelGold1, testLabelPredicted1)\n",
        "    results2 = confusion_matrix(testLabelGold2, testLabelPredicted2)\n",
        "\n",
        "    print('Confusion Matrix of HS:')\n",
        "    print(results)\n",
        "\n",
        "\n",
        "    print('Recall Score:', recall_score(testLabelGold, testLabelPredicted, average=\"macro\"))\n",
        "    print('Precision Score:', precision_score(testLabelGold, testLabelPredicted, average=\"macro\"))\n",
        "    print('F1 Score:', f1_score(testLabelGold, testLabelPredicted, average=\"macro\"))\n",
        "    print('Accuracy:', accuracy_score(testLabelGold, testLabelPredicted))\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print('Confusion Matrix of TR:')\n",
        "    print(results1)\n",
        "\n",
        "    print('Recall Score:', recall_score(testLabelGold1, testLabelPredicted1, average=\"macro\"))\n",
        "    print('Precision Score:', precision_score(testLabelGold1, testLabelPredicted1, average=\"macro\"))\n",
        "    print('F1 Score:', f1_score(testLabelGold1, testLabelPredicted1, average=\"macro\"))\n",
        "    print('Accuracy:', accuracy_score(testLabelGold1, testLabelPredicted1))\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print('Confusion Matrix of AG:')\n",
        "    print(results2)\n",
        "\n",
        "    print('Recall Score:', recall_score(testLabelGold2, testLabelPredicted2, average=\"macro\"))\n",
        "    print('Precision Score:', precision_score(testLabelGold2, testLabelPredicted2, average=\"macro\"))\n",
        "    print('F1 Score:', f1_score(testLabelGold2, testLabelPredicted2, average=\"macro\"))\n",
        "    print('Accuracy:', accuracy_score(testLabelGold2, testLabelPredicted2))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKcRYzO14poS",
        "outputId": "6f52a0b5-133b-476c-b92f-07f49f3be80d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "BERT embeddings:\n",
            " tensor([[-0.2812, -0.0708, -0.1025,  ..., -0.1670, -0.1111,  0.2747],\n",
            "        [ 0.0835,  0.1322, -0.2195,  ..., -0.1943, -0.1703,  0.2581],\n",
            "        [-0.1818, -0.0917, -0.2757,  ..., -0.2642, -0.0349,  0.1817],\n",
            "        ...,\n",
            "        [ 0.0187, -0.1545, -0.5340,  ..., -0.1533,  0.0364,  0.3309],\n",
            "        [ 0.2461, -0.1326, -0.1654,  ...,  0.1752, -0.1052,  0.0389],\n",
            "        [ 0.0791, -0.2272, -0.4102,  ..., -0.2416,  0.2388,  0.0708]])\n",
            "FastText embeddings:\n",
            " tensor([[ 0.0109, -0.0402,  0.0409,  ...,  0.0037,  0.0361,  0.0087],\n",
            "        [ 0.0157, -0.0077,  0.0299,  ...,  0.0172,  0.0011, -0.0064],\n",
            "        [ 0.0061,  0.0128,  0.0106,  ..., -0.0036, -0.0278, -0.0263],\n",
            "        ...,\n",
            "        [-0.0089,  0.0004,  0.0138,  ...,  0.0154,  0.0109, -0.0167],\n",
            "        [-0.0376,  0.0211,  0.0359,  ...,  0.0070,  0.0204, -0.0069],\n",
            "        [ 0.0092,  0.0294,  0.0310,  ..., -0.0146,  0.0085,  0.0034]],\n",
            "       dtype=torch.float64)\n",
            "GloVe embeddings:\n",
            " tensor([[ 0.2935, -0.0101,  0.0026,  ...,  0.1431,  0.1232, -0.2019],\n",
            "        [ 0.0622,  0.1375, -0.0562,  ...,  0.0963,  0.0559, -0.2459],\n",
            "        [-0.1550, -0.0344, -0.0644,  ..., -0.0358, -0.0120,  0.0160],\n",
            "        ...,\n",
            "        [-0.0141,  0.2304, -0.0901,  ...,  0.1505, -0.2612,  0.0312],\n",
            "        [-0.2466,  0.0939, -0.1703,  ...,  0.5293, -0.1125,  0.0061],\n",
            "        [-0.0399,  0.1914, -0.0556,  ...,  0.2032,  0.1070, -0.0642]],\n",
            "       dtype=torch.float64)\n",
            "Combined embeddings:\n",
            " tensor([[-0.2812, -0.0708, -0.1025,  ...,  0.1431,  0.1232, -0.2019],\n",
            "        [ 0.0835,  0.1322, -0.2195,  ...,  0.0963,  0.0559, -0.2459],\n",
            "        [-0.1818, -0.0917, -0.2757,  ..., -0.0358, -0.0120,  0.0160],\n",
            "        ...,\n",
            "        [ 0.0187, -0.1545, -0.5340,  ...,  0.1505, -0.2612,  0.0312],\n",
            "        [ 0.2461, -0.1326, -0.1654,  ...,  0.5293, -0.1125,  0.0061],\n",
            "        [ 0.0791, -0.2272, -0.4102,  ...,  0.2032,  0.1070, -0.0642]],\n",
            "       dtype=torch.float64)\n",
            "Scaled combined embeddings:\n",
            " [[-2.04357316 -0.46958909  0.75959068 ...  0.45823612  0.91054753\n",
            "  -0.77581792]\n",
            " [ 0.2153102   0.83294192  0.06949343 ...  0.13502317  0.48524627\n",
            "  -1.05141558]\n",
            " [-1.42779279 -0.60365159 -0.26200281 ... -0.77791123  0.05546869\n",
            "   0.58955634]\n",
            " ...\n",
            " [-0.18610593 -1.00682045 -1.78602911 ...  0.50920454 -1.5199652\n",
            "   0.68521739]\n",
            " [ 1.22235808 -0.86625009  0.38828773 ...  3.12586922 -0.57989333\n",
            "   0.52777977]\n",
            " [ 0.18798468 -1.47358596 -1.0553929  ...  0.87320275  0.80802688\n",
            "   0.08704991]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (32) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (32) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (32) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Complete test data reading .....\n",
            "BERT embeddings:\n",
            " tensor([[ 0.4375, -0.2203, -0.1798,  ..., -0.0187, -0.4133,  0.3171],\n",
            "        [ 0.3901,  0.0463, -0.2264,  ...,  0.1144,  0.0545, -0.1429],\n",
            "        [ 0.1146, -0.0159, -0.4748,  ..., -0.0878, -0.3197,  0.3748],\n",
            "        ...,\n",
            "        [ 0.2389,  0.0132, -0.0268,  ..., -0.0615, -0.2744,  0.2419],\n",
            "        [ 0.1472, -0.0367, -0.0779,  ..., -0.1592, -0.2151,  0.3098],\n",
            "        [-0.2325,  0.0397, -0.1185,  ...,  0.4342, -0.7840, -0.5044]])\n",
            "FastText embeddings:\n",
            " tensor([[-0.0128, -0.0490,  0.0164,  ...,  0.0315,  0.0032, -0.0354],\n",
            "        [-0.0109, -0.0283,  0.0077,  ...,  0.0088,  0.0198,  0.0025],\n",
            "        [-0.0557, -0.0340,  0.0769,  ..., -0.1092,  0.0821,  0.0877],\n",
            "        ...,\n",
            "        [ 0.0151, -0.0153,  0.0052,  ...,  0.0173, -0.0173,  0.0431],\n",
            "        [-0.0097, -0.0141, -0.0106,  ..., -0.0158, -0.0149, -0.0038],\n",
            "        [-0.0516, -0.0110, -0.0048,  ...,  0.0152,  0.0166, -0.0009]],\n",
            "       dtype=torch.float64)\n",
            "GloVe embeddings:\n",
            " tensor([[ 0.1979,  0.0227, -0.0539,  ...,  0.1009, -0.4737,  0.3997],\n",
            "        [-0.0180,  0.0906, -0.1753,  ...,  0.1649,  0.0703, -0.0212],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        ...,\n",
            "        [ 0.1313,  0.2494, -0.2866,  ..., -0.0310,  0.0715, -0.1687],\n",
            "        [ 0.2411, -0.0202,  0.0282,  ...,  0.2526, -0.0490,  0.4662],\n",
            "        [-0.0208,  0.2310, -0.0544,  ...,  0.0046,  0.3105, -0.1305]],\n",
            "       dtype=torch.float64)\n",
            "Combined test embeddings:\n",
            " tensor([[ 0.4375, -0.2203, -0.1798,  ...,  0.1009, -0.4737,  0.3997],\n",
            "        [ 0.3901,  0.0463, -0.2264,  ...,  0.1649,  0.0703, -0.0212],\n",
            "        [ 0.1146, -0.0159, -0.4748,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        ...,\n",
            "        [ 0.2389,  0.0132, -0.0268,  ..., -0.0310,  0.0715, -0.1687],\n",
            "        [ 0.1472, -0.0367, -0.0779,  ...,  0.2526, -0.0490,  0.4662],\n",
            "        [-0.2325,  0.0397, -0.1185,  ...,  0.0046,  0.3105, -0.1305]],\n",
            "       dtype=torch.float64)\n",
            "Scaled combined test embeddings:\n",
            " [[ 2.40772794 -1.4288656   0.30340901 ...  0.16672509 -2.86400214\n",
            "   2.99436842]\n",
            " [ 2.11456539  0.28148661  0.02889646 ...  0.60832638  0.57595671\n",
            "   0.35652199]\n",
            " [ 0.40792831 -0.11781116 -1.43637587 ... -0.5303656   0.13165653\n",
            "   0.48959705]\n",
            " ...\n",
            " [ 1.17806616  0.06898537  1.20623497 ... -0.74417069  0.5837659\n",
            "  -0.56776232]\n",
            " [ 0.61007687 -0.25090664  0.90447009 ...  1.21463895 -0.17799572\n",
            "   3.41080829]\n",
            " [-1.74202864  0.23908149  0.66495543 ... -0.4988861   2.09514526\n",
            "  -0.32842605]]\n",
            "Classification Report of HS:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     hateful       0.49      0.83      0.61      1252\n",
            "  notHateful       0.75      0.36      0.49      1719\n",
            "\n",
            "    accuracy                           0.56      2971\n",
            "   macro avg       0.62      0.60      0.55      2971\n",
            "weighted avg       0.64      0.56      0.54      2971\n",
            "\n",
            "Confusion Matrix of HS:\n",
            "[[1041  211]\n",
            " [1097  622]]\n",
            "Recall Score: 0.5966539633154724\n",
            "Precision Score: 0.6168011638705997\n",
            "F1 Score: 0.5508100535412102\n",
            "Accuracy: 0.5597441938741164\n",
            "\n",
            "\n",
            "Confusion Matrix of TR:\n",
            "[[2022  427]\n",
            " [ 185  337]]\n",
            "Recall Score: 0.7356184946862352\n",
            "Precision Score: 0.6786376403494829\n",
            "F1 Score: 0.6963312276538776\n",
            "Accuracy: 0.7940087512622013\n",
            "\n",
            "\n",
            "Confusion Matrix of AG:\n",
            "[[ 259  331]\n",
            " [ 530 1851]]\n",
            "Recall Score: 0.6081937513792097\n",
            "Precision Score: 0.5882839664079536\n",
            "F1 Score: 0.5934714337681928\n",
            "Accuracy: 0.7101985863345674\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "CAlrFza8yiui",
        "vMTIC5GokjRz",
        "p6pzCkCnz-40",
        "bqRMvujWlBnZ",
        "b6Wlt1hzuGjo",
        "-nVwzY10vvkG",
        "b23FE34A6zEy"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "033b523b938d4720be2173a8107d7acf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d0c19deb2284e78bd69528b4df03005": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0d2584d0fab84abf8800b28505349684": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0d712dbc37f14c3ea21923cece4c91d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_473f38a269834c369f33af407f9d73d3",
            "placeholder": "​",
            "style": "IPY_MODEL_8b97e6b8a432480598b4d3e32e2e7a19",
            "value": "tokenizer.json: 100%"
          }
        },
        "1c245728fb3641e3bc7d72608deba63c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c5d7e3bff2f4b8d906858492a10568f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_999f85da94d248018c655b518f78e3bb",
            "placeholder": "​",
            "style": "IPY_MODEL_8af9878d5fa141c3b19cb19e13508fe2",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "1d33620fb1e840db8be0eb642e2dcf29": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "211b04c9c15446bbbb88ff9e4c9208b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0d712dbc37f14c3ea21923cece4c91d2",
              "IPY_MODEL_fa65980d6d234f79a98a950e1a4b8b41",
              "IPY_MODEL_9a038f2aa1e9441da197886d019a49c6"
            ],
            "layout": "IPY_MODEL_469c05b9d53a4441a0a872ac5ea927fe"
          }
        },
        "23189e118de4495eb98035b14496d92f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "278da640f40947b992f4fadf35de8b98": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_60ca1128414c468aaca980aa762ef753",
              "IPY_MODEL_f116126af6ad4c239278e304a78c5425",
              "IPY_MODEL_6594e621fd304deaad83c0ca536b12fe"
            ],
            "layout": "IPY_MODEL_3563b45366174366a3b482ea6e797e0c"
          }
        },
        "27bb3370327b41a796941ae385955a3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3563b45366174366a3b482ea6e797e0c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39d3fc4d5ab34fa7893a4c4d12ec77c3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "407093cf3ac14872849c8fae711ee35e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "469c05b9d53a4441a0a872ac5ea927fe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "473f38a269834c369f33af407f9d73d3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4dc716daf9bc4c98be01cfcf35b4067d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9035289b8988466ea6311011d73ed585",
            "placeholder": "​",
            "style": "IPY_MODEL_749c7d7eb9344323921bae86ab05268f",
            "value": " 232k/232k [00:00&lt;00:00, 3.65MB/s]"
          }
        },
        "50680eceddb2412580f1b5466a0dba9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f5641ec9e584d9daf20ba5a8bdc372e",
            "max": 1344951957,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d2584d0fab84abf8800b28505349684",
            "value": 1344951957
          }
        },
        "562bb5fed45942c3998146c28f0ebec7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2e38f8002494959baa138e8bd6daf4d",
            "placeholder": "​",
            "style": "IPY_MODEL_27bb3370327b41a796941ae385955a3d",
            "value": " 48.0/48.0 [00:00&lt;00:00, 3.25kB/s]"
          }
        },
        "5dc51983f8104ab4b903ddb9902f6015": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60ca1128414c468aaca980aa762ef753": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f97281a11fe47a0a92ba705bb2c4e9a",
            "placeholder": "​",
            "style": "IPY_MODEL_c9c28c06babd40838aff9a21b8673944",
            "value": "config.json: 100%"
          }
        },
        "6594e621fd304deaad83c0ca536b12fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c245728fb3641e3bc7d72608deba63c",
            "placeholder": "​",
            "style": "IPY_MODEL_dfdf8c79bf6b4f40b57a85fd631455bd",
            "value": " 571/571 [00:00&lt;00:00, 25.0kB/s]"
          }
        },
        "6acb83090aca40e1aa45f53c05003818": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c95488e5d08488db44f3ced1514a0f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1c5d7e3bff2f4b8d906858492a10568f",
              "IPY_MODEL_80594434db724cf691cde24cb424ba70",
              "IPY_MODEL_562bb5fed45942c3998146c28f0ebec7"
            ],
            "layout": "IPY_MODEL_b698f79d9a6a48d5ab6c73a564b04bf2"
          }
        },
        "6f5641ec9e584d9daf20ba5a8bdc372e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "749c7d7eb9344323921bae86ab05268f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7613983e6f43489d80e95fa193828be0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80594434db724cf691cde24cb424ba70": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d33620fb1e840db8be0eb642e2dcf29",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_23189e118de4495eb98035b14496d92f",
            "value": 48
          }
        },
        "81829d9430084411b12c6c7ab4c41844": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "87c52b363d234080bee5e8267b4836e7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8af9878d5fa141c3b19cb19e13508fe2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b97e6b8a432480598b4d3e32e2e7a19": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e03812b15bd4f539f0a566d8fbd0b07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9035289b8988466ea6311011d73ed585": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "905563cf8f7a4398aad87d80dafd8dff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7613983e6f43489d80e95fa193828be0",
            "placeholder": "​",
            "style": "IPY_MODEL_81829d9430084411b12c6c7ab4c41844",
            "value": " 1.34G/1.34G [00:12&lt;00:00, 72.8MB/s]"
          }
        },
        "928906f15d8c4245b5438b9b302774f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "944fd0cf6dc64769b5443a9c1c6b7af1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6acb83090aca40e1aa45f53c05003818",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_407093cf3ac14872849c8fae711ee35e",
            "value": 231508
          }
        },
        "999f85da94d248018c655b518f78e3bb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a038f2aa1e9441da197886d019a49c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d12d4f1fabf4457f97f13fc37de3a7b2",
            "placeholder": "​",
            "style": "IPY_MODEL_033b523b938d4720be2173a8107d7acf",
            "value": " 466k/466k [00:00&lt;00:00, 3.46MB/s]"
          }
        },
        "9a7fb4d27878421c970e11d8e7b1333a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5dc51983f8104ab4b903ddb9902f6015",
            "placeholder": "​",
            "style": "IPY_MODEL_f2075f9507c3432e8f430256a857739b",
            "value": "vocab.txt: 100%"
          }
        },
        "9f97281a11fe47a0a92ba705bb2c4e9a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af1b06e2f91e4020ab6a38536a62fb4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e85c7798ee1b42f9984cddeddf0a2c9c",
              "IPY_MODEL_50680eceddb2412580f1b5466a0dba9f",
              "IPY_MODEL_905563cf8f7a4398aad87d80dafd8dff"
            ],
            "layout": "IPY_MODEL_39d3fc4d5ab34fa7893a4c4d12ec77c3"
          }
        },
        "b3067447ea2b400ca2afe284ebbcbee1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9a7fb4d27878421c970e11d8e7b1333a",
              "IPY_MODEL_944fd0cf6dc64769b5443a9c1c6b7af1",
              "IPY_MODEL_4dc716daf9bc4c98be01cfcf35b4067d"
            ],
            "layout": "IPY_MODEL_c10ce5367cc747bdabae7c46941d421a"
          }
        },
        "b698f79d9a6a48d5ab6c73a564b04bf2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c10ce5367cc747bdabae7c46941d421a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9c28c06babd40838aff9a21b8673944": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d12d4f1fabf4457f97f13fc37de3a7b2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfdf8c79bf6b4f40b57a85fd631455bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e85c7798ee1b42f9984cddeddf0a2c9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ffd3286c8c474acfac736ecc91c17c5d",
            "placeholder": "​",
            "style": "IPY_MODEL_8e03812b15bd4f539f0a566d8fbd0b07",
            "value": "model.safetensors: 100%"
          }
        },
        "eb55153ab1db454fb9c2444e8e9888b2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f116126af6ad4c239278e304a78c5425": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb55153ab1db454fb9c2444e8e9888b2",
            "max": 571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d0c19deb2284e78bd69528b4df03005",
            "value": 571
          }
        },
        "f2075f9507c3432e8f430256a857739b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2e38f8002494959baa138e8bd6daf4d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa65980d6d234f79a98a950e1a4b8b41": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87c52b363d234080bee5e8267b4836e7",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_928906f15d8c4245b5438b9b302774f4",
            "value": 466062
          }
        },
        "ffd3286c8c474acfac736ecc91c17c5d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78859695a1eb4eb99ba00b9f92d88aa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_28a45ab5564b432baf3f0a002b772c6e",
              "IPY_MODEL_8c06ae1e4a0e4c138a873edf8795b224",
              "IPY_MODEL_a5eff5777cf240d4af862cdc37b476c1"
            ],
            "layout": "IPY_MODEL_4d0492bacdeb4176b2a479b7a82dd419"
          }
        },
        "28a45ab5564b432baf3f0a002b772c6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62959bc6198a45198b504ad3e21dd0c6",
            "placeholder": "​",
            "style": "IPY_MODEL_5e5a84277a8e41b6bb45f86a6a414d68",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "8c06ae1e4a0e4c138a873edf8795b224": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d2a0383a4044b3eb6db2c7599d9f652",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_157d3e5c096b40369fec4c81f7f72c2d",
            "value": 48
          }
        },
        "a5eff5777cf240d4af862cdc37b476c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1aa5a3c9d82428bb882fc5ad79116a2",
            "placeholder": "​",
            "style": "IPY_MODEL_9898ffa65f2944b5a439060b406ddca7",
            "value": " 48.0/48.0 [00:00&lt;00:00, 3.47kB/s]"
          }
        },
        "4d0492bacdeb4176b2a479b7a82dd419": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62959bc6198a45198b504ad3e21dd0c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e5a84277a8e41b6bb45f86a6a414d68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d2a0383a4044b3eb6db2c7599d9f652": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "157d3e5c096b40369fec4c81f7f72c2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d1aa5a3c9d82428bb882fc5ad79116a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9898ffa65f2944b5a439060b406ddca7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "84dad553b8ab4c8b97c62efeda488080": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_23abfc1bfec7436b983c6a4edb25d865",
              "IPY_MODEL_c2add516222a428aa4a698db669ae7f7",
              "IPY_MODEL_d8ed91be0f1a405599a3a637f24af726"
            ],
            "layout": "IPY_MODEL_8a2c174eb6614ed3afa4ca6b899dec50"
          }
        },
        "23abfc1bfec7436b983c6a4edb25d865": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ca28968107348c4ad8a4f3cb3f6bd9b",
            "placeholder": "​",
            "style": "IPY_MODEL_7fe564bf8ee149e4bcfa594686aecf49",
            "value": "vocab.txt: 100%"
          }
        },
        "c2add516222a428aa4a698db669ae7f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e11d4242d5d74490837e3079e3826c35",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6da507ecc33a4b1e9d0b3b3ea8faa511",
            "value": 231508
          }
        },
        "d8ed91be0f1a405599a3a637f24af726": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe8c458b085c4e42b5f7e71bf1eeb562",
            "placeholder": "​",
            "style": "IPY_MODEL_bcc8c91e671c4a898665772a01fc6a6e",
            "value": " 232k/232k [00:00&lt;00:00, 4.97MB/s]"
          }
        },
        "8a2c174eb6614ed3afa4ca6b899dec50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ca28968107348c4ad8a4f3cb3f6bd9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fe564bf8ee149e4bcfa594686aecf49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e11d4242d5d74490837e3079e3826c35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6da507ecc33a4b1e9d0b3b3ea8faa511": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fe8c458b085c4e42b5f7e71bf1eeb562": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcc8c91e671c4a898665772a01fc6a6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c008e6b848b449f6b9f2b166a9a510e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a42e301e776b41009f2f5e75f37d85aa",
              "IPY_MODEL_f9b769aa11b7444993fe79a199ba6bcf",
              "IPY_MODEL_fe54535654424fb386e8a679c990be26"
            ],
            "layout": "IPY_MODEL_f9d2dbbd68ed4ebcbb76a923e5929c43"
          }
        },
        "a42e301e776b41009f2f5e75f37d85aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5bebbb022d14bf785e1757912e8b027",
            "placeholder": "​",
            "style": "IPY_MODEL_1ba07fed045847288e9c55e4dcde8cf3",
            "value": "tokenizer.json: 100%"
          }
        },
        "f9b769aa11b7444993fe79a199ba6bcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab534ac42b574d69a4c6451a07783d1d",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_61e8bdb5fab843adb0351042e8503338",
            "value": 466062
          }
        },
        "fe54535654424fb386e8a679c990be26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1e4b6e38577401096a03c7f0eac467c",
            "placeholder": "​",
            "style": "IPY_MODEL_013e8f736de9432c8155b09ea3304dd2",
            "value": " 466k/466k [00:00&lt;00:00, 9.45MB/s]"
          }
        },
        "f9d2dbbd68ed4ebcbb76a923e5929c43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5bebbb022d14bf785e1757912e8b027": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ba07fed045847288e9c55e4dcde8cf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab534ac42b574d69a4c6451a07783d1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61e8bdb5fab843adb0351042e8503338": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d1e4b6e38577401096a03c7f0eac467c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "013e8f736de9432c8155b09ea3304dd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "12b1960c16a04901be21e9f053076ecb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0047571ab5cb4d578396cca13cb22953",
              "IPY_MODEL_8a191bda980b47998b35eb0287601df7",
              "IPY_MODEL_c793c09d6e7244abaad69b4d575d33b7"
            ],
            "layout": "IPY_MODEL_efaff630771a4901809b55aefdd22ca3"
          }
        },
        "0047571ab5cb4d578396cca13cb22953": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d26c271ca1b469c866f7cfd534abaf1",
            "placeholder": "​",
            "style": "IPY_MODEL_78a5411860554259bc073d33053072db",
            "value": "config.json: 100%"
          }
        },
        "8a191bda980b47998b35eb0287601df7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59675dad38b341b7aa329a5417548d65",
            "max": 571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a1160d6f0a4a4afcb0fc5292f1f656dd",
            "value": 571
          }
        },
        "c793c09d6e7244abaad69b4d575d33b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f66be7c7dbec41aba9d9d240409d0a56",
            "placeholder": "​",
            "style": "IPY_MODEL_26c1cca17e2440b3a9f2771e33231f10",
            "value": " 571/571 [00:00&lt;00:00, 34.5kB/s]"
          }
        },
        "efaff630771a4901809b55aefdd22ca3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d26c271ca1b469c866f7cfd534abaf1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78a5411860554259bc073d33053072db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59675dad38b341b7aa329a5417548d65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1160d6f0a4a4afcb0fc5292f1f656dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f66be7c7dbec41aba9d9d240409d0a56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26c1cca17e2440b3a9f2771e33231f10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5fd969b6abb344829f808b272c3379bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c911fc4cd9ee4d5abe4d25dcfe9922d6",
              "IPY_MODEL_6ac9466d660743d58e756516e4fb24ec",
              "IPY_MODEL_56f3e75190f4474b92729221b25599d7"
            ],
            "layout": "IPY_MODEL_088e16bac72f4ff888c55afebd5afc07"
          }
        },
        "c911fc4cd9ee4d5abe4d25dcfe9922d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea771a1907484dfa977f3d8add4fa6e3",
            "placeholder": "​",
            "style": "IPY_MODEL_c559b10c408c4717b270bf4a1fc2d3fe",
            "value": "model.safetensors: 100%"
          }
        },
        "6ac9466d660743d58e756516e4fb24ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c95d18e786d24e1ba555efa2b8cb9e66",
            "max": 1344951957,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_48c7dbab5b30441eab8b8222b32323fb",
            "value": 1344951957
          }
        },
        "56f3e75190f4474b92729221b25599d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d139d59447bc468bb6bed86adc54e583",
            "placeholder": "​",
            "style": "IPY_MODEL_c2a21be083b94280ba181c371cad5583",
            "value": " 1.34G/1.34G [00:08&lt;00:00, 145MB/s]"
          }
        },
        "088e16bac72f4ff888c55afebd5afc07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea771a1907484dfa977f3d8add4fa6e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c559b10c408c4717b270bf4a1fc2d3fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c95d18e786d24e1ba555efa2b8cb9e66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48c7dbab5b30441eab8b8222b32323fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d139d59447bc468bb6bed86adc54e583": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2a21be083b94280ba181c371cad5583": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}